---
title: "Poisson Regression Examples"
author: "Yoonjeong Park"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty develops software that streamlines the blueprint drawings required for U.S. patent filings. The company’s marketing team suspects that firms using the tool see higher patent‑approval success than those that don’t. Ideally, we would compare each firm’s approval rate before and after adopting Blueprinty, but that historical view isn’t on file.

Instead, we have a snapshot of 1,500 established engineering firms. For every company we know the number of patents awarded in the past five years, its region, age, and whether it licenses Blueprinty. By modeling these data we can isolate the impact of Blueprinty usage—controlling for basic firm attributes—and see whether the software really delivers a measurable edge in winning patents.


### Data Overview

Before estimating any models, we first look at the raw Blueprinty dataset to understand how customer firms differ from non‑customers. This section walks through three quick diagnostics: (a) comparing patent output (b) checking whether observable firm traits—region (c) age—line up across the two groups. These checks set expectations for the Poisson regressions we will fit later.

```{python}
#| label: load-data
#| include: false
#| echo: true        
#| output: false   
#| results: hide   
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# corporate colors
BLUE  = "#6CA0DC"
PURPLE = "#B19CD9"
plt.rcParams["axes.prop_cycle"] = plt.cycler(color=[BLUE, PURPLE])

df = pd.read_csv("/Users/yoonjeong_park/mysite/_data/blueprinty.csv")   # adjust path if needed
df.head()
```

#### 1.1 Comparing Patent Output
 
```{python}
#| label: Comparing Patent Output
#| code-fold: true
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator

df = pd.read_csv("/Users/yoonjeong_park/mysite/_data/blueprinty.csv")   # adjust 

# corporate palette
BLUE   = "#6CA0DC"   # customers
PURPLE = "#B19CD9"   # non‑customers

# ----- prepare counts -------------------------------------------------
bins      = np.arange(df["patents"].min(), df["patents"].max() + 1)        # 0 … 16
cust_cnt  = (df.loc[df["iscustomer"] == 1, "patents"]
               .value_counts()
               .reindex(bins, fill_value=0)
               .sort_index())
non_cnt   = (df.loc[df["iscustomer"] == 0, "patents"]
               .value_counts()
               .reindex(bins, fill_value=0)
               .sort_index())

# ----- plot ------------------------------------------------------------
x      = bins
width  = 0.4                       # half‑bin width

fig, ax = plt.subplots(figsize=(7, 4))

ax.bar(x - width/2, cust_cnt.values,
       width=width, color=BLUE,   label="Customer")
ax.bar(x + width/2, non_cnt.values,
       width=width, color=PURPLE, label="Non‑customer")

# nice, integer axes
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
ax.yaxis.set_major_locator(MaxNLocator(integer=True))

ax.set_xlabel("Number of patents")
ax.set_ylabel("Count")
ax.set_title("Distribution of Number of Patents")
ax.legend(frameon=False)
plt.show()
```

The distribution of number of patents shows a clear picture:

* **Customers file a bit more.** Most Blueprinty users sit around 4 patents and a few reach double digits.  
* **Non-customers stay low.** Firms that don’t use the software pile up at 0–2 patents and rarely pass 6.  
* **Still plenty of overlap.** The majority of all firms hold fewer than 6 patents, so the two groups aren’t miles apart.


Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

#### 1.2 How are customers spread across regions?
```{python}
#| label: plot-patent-dist
#| echo: false
bins = np.arange(df["patents"].min(), df["patents"].max() + 1)

counts = (
    df.groupby(["patents", "iscustomer"]).size()
      .unstack(fill_value=0)              
      .reindex(bins)                          
      .rename(columns={0: "Non-customer", 1: "Customer"})
)

fig, ax = plt.subplots(figsize=(7, 4))
width = 0.4

ax.bar(bins - width/2, counts["Customer"],     width, color=BLUE,   label="Customer")
ax.bar(bins + width/2, counts["Non-customer"], width, color=PURPLE, label="Non-customer")

ax.xaxis.set_major_locator(MaxNLocator(integer=True))
ax.yaxis.set_major_locator(MaxNLocator(integer=True))
ax.set_xlabel("Number of patents")
ax.set_ylabel("Count")
ax.set_title("Distribution of Number of Patents")
ax.legend(frameon=False)
plt.tight_layout()
plt.show()
```

The bars plot say a lot about where Blueprinty’s users come from:

* **Northeast is the hotspot.** It hosts the biggest share of all firms and, notably, an even larger share of customers.  
* **Everywhere else skews non-customer.** Midwest, Northwest, South, and Southwest each have many more non-customers than customers—sometimes five-to-one.  
* **Room to grow.** Those non-customer-heavy regions hint at obvious white-space for the sales team.

#### 1.3 Age line up across the two groups
```{python}
#| label: age-dist
#| code-fold: true
#| code-summary: "Age distribution by customer status"
# ── Split ages by status ───────────────────────────────────────
age_cust = df.loc[df.iscustomer == 1, 'age']
age_non  = df.loc[df.iscustomer == 0, 'age']

# ── Overlapping histograms ─────────────────────────────────────
bins = np.arange(0, df['age'].max() + 5, 5)     # 5-year bins

plt.figure(figsize=(7, 4))
plt.hist(age_cust, bins=bins, alpha=0.7, color='#6CA0DC', label='Customer')
plt.hist(age_non,  bins=bins, alpha=0.7, color='#B19CD9', label='Non-customer')

plt.xlabel('Firm age (years)')
plt.ylabel('Number of firms')
plt.title('Age Distribution by Customer Status')
plt.legend(title='Status')
plt.show()
```

If region separates customers from non-customers, age clearly does not:

* **Nearly identical curves.** Both segments cluster in the 15-35-year window, with peaks around the mid-20s. You can barely tell one histogram from the other.  
* **Few outliers.** Only a sprinkling of firms—customer or not—make it past 40 years. The extremes don’t tilt toward either side.

Firm age doesn’t look like a deciding factor in adopting Blueprinty. When we estimate the Poisson model, we’ll control for age to be safe, but we don’t expect it to change the customer effect we saw in regions.

### Estimation of Simple Poisson Model
Since our outcome, the count of patents in the last five years is a non-negative integer, the Poisson distribution is a natural starting point. We begin with the most stripped-down case: one common rate parameter, λ, for every firm and estimate it by Maximum Likelihood.

#### 2.1 Likelihood for the Poisson
For a single firm  

$$
Y_i \sim \text{Poisson}(\lambda)
\quad\Longrightarrow\quad
f(Y_i\mid\lambda)
=\frac{e^{-\lambda}\,\lambda^{Y_i}}{Y_i!}.
$$

With $n$ independent firms the joint likelihood is the product of those densities:

$$
\mathcal{L}(\lambda;\mathbf y)
=\prod_{i=1}^{n}\frac{e^{-\lambda}\,\lambda^{y_i}}{y_i!}
=e^{-n\lambda}\,
  \lambda^{\sum_{i=1}^{n}y_i}\Big/\!\prod_{i=1}^{n}y_i!.
$$

The log‑likelihood derived above is easy to translate into Python. All we need is (i) the **rate parameter** `lmbda` and (ii) an array of **observed counts** `y`.
```{python}
#| label: Likelihood function for Poisson model
#| code-fold: false
from scipy.special import gammaln   

def poisson_loglikelihood(lmbda: float, y: np.ndarray) -> float:
    y = np.asarray(y)
    n = y.size
    return -n * lmbda + y.sum() * np.log(lmbda) - gammaln(y + 1).sum()
```

#### 2.2  A quick look at the likelihood surface 
Before trusting the optimiser, it’s good practice to see the curve we are maximising. Below I feed the observed patent counts into `poisson_loglikelihood`, evaluate it on a grid of candidate λ’s, and plot the log-likelihood. The peak should line up with the MLE we just computed.

```{python}
#| label: pois-like-plot
#| echo: true
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt

# --- data & MLE from the previous step ---------------------------------
y_data = df['patents'].values
lambda_hat = y_data.mean()                   # analytic MLE

# --- grid of λ values ---------------------------------------------------
lam_grid = np.linspace(0.1, 8, 200)          # adjust max if needed
loglik_vals = [poisson_loglikelihood(l, y_data) for l in lam_grid]

# --- plot ---------------------------------------------------------------
plt.figure(figsize=(6, 4))
plt.plot(lam_grid, loglik_vals, color='#6CA0DC', lw=2)
plt.axvline(lambda_hat, color='#B19CD9', ls='--', lw=1.5,
            label=f'λ̂ = {lambda_hat:.2f}')
plt.xlabel('λ (candidate rate)')
plt.ylabel('Log-likelihood  ℓ(λ)')
plt.title('Log-likelihood profile for the simple Poisson model')
plt.legend()
plt.tight_layout()
plt.show()
```

* **Single, smooth peak.** The log-likelihood rises sharply, tops out near **λ̂ ≈ 3.7**, and then drifts downward—exactly what we expect from a concave, well-behaved Poisson likelihood.  
* **MLE matches intuition.** The peak sits almost exactly at the sample mean of the patent counts, foreshadowing the analytic result we’ll confirm next.  
* **Numerical stability.** The curve is steep for very small λ but flattens around the optimum, so any reasonable optimiser will land in the same neighbourhood.

Next we prove this formally: taking ∂ℓ/∂λ, setting it to zero, and showing the solution is the sample mean **Ŷ = λ̂**.

#### 2.3 Solving for the MLE by hand 
Starting from the log-likelihood

$$[
\ell(\lambda)
   =-n\lambda
    +\bigl(\textstyle\sum_{i=1}^{n}y_i\bigr)\log\lambda
    -\sum_{i=1}^{n}\log(y_i!)]
$$

take the first derivative:

$$[\frac{\partial \ell}{\partial\lambda}
   = -n + \frac{\sum_{i=1}^{n}y_i}{\lambda}]$$

Set the derivative to zero and solve for λ:

$$[ -n + \frac{\sum y_i}{\lambda}=0
\quad\Longrightarrow\quad
\hat\lambda
  =\frac{1}{n}\sum_{i=1}^{n}y_i
  =\bar Y]$$

Because the second derivative is negative, this critical point is indeed a maximum.

We can check this in Python as well.

```{python}
#| label: pois-mle-analytic
#| echo: true
#| code-fold: true
y_data = df['patents'].values
lambda_mean = y_data.mean()          

lambda_grid_peak = lambda_hat        

print(f"Closed-form  λ̂ = {lambda_mean:.4f}")
print(f"Grid/optimiser λ̂ = {lambda_grid_peak:.4f}")
```
Both paths land on the same estimate, confirming that the sample mean is the Maximum-Likelihood estimate for this simple Poisson model.

#### 2.4 Finding MLE by optimizing my likelihood function 
To close the loop we ask SciPy’s optimiser to find the λ that maximises our `poisson_loglikelihood`. (We tell it to minimise the negative log-likelihood.)

```{python}
#| label: pois-mle-opt
#| echo: true
from scipy.optimize import minimize_scalar

neg_ll = lambda lmbda: -poisson_loglikelihood(lmbda, y_data)
opt = minimize_scalar(neg_ll, bounds=(1e-6, 20), method='bounded')

lambda_opt  = opt.x
loglik_opt  = -opt.fun

print(f"Optimiser  λ̂ = {lambda_opt:.4f}")
print(f"Log-lik at optimum = {loglik_opt:.2f}")
print(f"Closed-form λ̂     = {lambda_mean:.4f}") 
```
The optimiser lands at λ̂ = Ȳ and produces the same log-likelihood value we computed manually. This confirms that both analytical calculus and numerical search point to one maximum: the sample mean. With the basic Poisson model nailed down, we can now move on to richer specifications where λ varies with firm characteristics.



### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

#### 3.1  Log-likelihood function with covariates
```{python}
#| label: pois-reg-loglik
#| echo: true
from scipy.special import gammaln   

def poisson_reg_loglik(beta: np.ndarray,
                       y:   np.ndarray,
                       X:   np.ndarray) -> float:

    y = np.asarray(y)
    eta = X @ beta              
    lam = np.exp(eta)              
    
    return (y * eta - lam - gammaln(y + 1)).sum()
```
The new function is nothing more than the “one-λ” log-likelihood from Section 2, applied **row-wise** with firm-specific rates:

* `η = X @ β` is the linear score for each firm.  
* `λ = exp(η)` enforces positivity and makes each coefficient a multiplicative effect:  
  a one-unit bump in a covariate scales expected patents by `exp(β_j)`.  
* Summing the Poisson kernel across firms gives the total log-likelihood.

#### 3.2  Maximising the regression log-likelihood  

We now assemble the design matrix **X** (intercept + age + age² + region dummies + customer flag) and let SciPy’s BFGS optimiser hunt for the β̂ that maximises `poisson_reg_loglik`.  
BFGS also returns an approximation to the inverse Hessian, which we use as an estimate of the coefficient variance–covariance matrix.

```{python}
#| label: pois-reg-mle
#| echo: true
#| code-fold: true
import pandas as pd
import numpy as np
from scipy.optimize import minimize

# ── log-likelihood, gradient, Hessian ─────────────────────────
def pois_reg_components(beta, y, X):
    """Return ℓ, ∇ℓ, and −H (Fisher info) for Poisson regression."""
    eta = X @ beta
    lam = np.exp(np.clip(eta, -20, 20))        # overflow guard
    ll  = (y * eta - lam - gammaln(y + 1)).sum()
    grad = X.T @ (y - lam)
    hess = -(X.T * lam) @ X                    # negative definite
    return ll, grad, hess

def neg_ll_grad_hess(beta, y, X):
    ll, grad, hess = pois_reg_components(beta, y, X)
    return -ll, -grad, -hess                   # minimise −ℓ

# ── design matrix (intercept + covariates) ────────────────────
X_df = pd.DataFrame({
    'intercept' : 1.0,
    'age'       : df['age'],
    'age_sq'    : df['age']**2,
    'customer'  : df['iscustomer']
})
X_df = pd.concat([X_df,
                  pd.get_dummies(df['region'], drop_first=True)], axis=1)

y_vec, X_mat = df['patents'].values, X_df.values
p            = X_mat.shape[1]

# ── optimise using Newton–CG with analytic pieces ─────────────
init_beta = np.zeros(p)

opt = minimize(lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[0],
               init_beta,
               method='Newton-CG',
               jac=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[1],
               hess=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[2],
               options={'xtol': 1e-8, 'disp': False})

if not opt.success:
    raise RuntimeError(opt.message)

beta_hat = opt.x
# covariance ≈ (−H)^−1 at optimum
_, _, hess_opt = pois_reg_components(beta_hat, y_vec, X_mat)
cov_hat = np.linalg.inv(-hess_opt)
se_hat  = np.sqrt(np.diag(cov_hat))

results = (
    pd.DataFrame({'coef': beta_hat, 'std_err': se_hat},
                 index=X_df.columns)
    .round(4)
)
results
```

The table below translates directly into multiplicative rate effects because our link is log-linear ( λ = exp (Xβ) ). 

* **Intercept (-0.1483).**  
  Baseline firm — Midwest, non-customer, age = 0 — is expected to file `exp(−0.15) ≈ 0.86` patents in five years.

* **Age terms (0.1225 and –0.0025).**  
  A one-year increase raises the log rate by 0.1225 but the negative squared term drags it down, creating an inverted-U. 

* **Customer flag (0.1870).**  
  Holding everything else constant, Blueprinty clients file `exp(0.187) ≈ 1.21` times — 21 % more** — patents than non-clients.

* **Region effects** (relative to Midwest).  
  All point estimates are small (±3 %) and their standard errors are of similar magnitude → no strong regional tilt once we control for age and customer status.


Most coefficients have |t| > 2 (β / SE), so the age dynamics and the customer bump are statistically meaningful, whereas regional differences are not.

#### 3.3 Checking results with `statsmodels.GLM`
To be sure our hand-rolled MLE matches a mature library, we refit the exact same design matrix with `statsmodels`’ Poisson GLM and compare coefficients.

```{python}
#| label: pois-reg-glm
#| echo: true
#| code-fold: true
import statsmodels.api as sm

glm_poiss = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())
fit       = glm_poiss.fit()

glm_table = (
    pd.DataFrame({
        'coef_glm' : fit.params,
        'se_glm'   : fit.bse,
        'coef_mle' : beta_hat,     
        'se_mle'   : se_hat
    }, index=X_df.columns)
    .round(4)
)
glm_table

```

The GLM check confirms our custom MLE, so we can interpret the estimates with confidence.

* **Blueprinty pays off.**  
  The `customer` coefficient is 0.19 (SE ≈ 0.03).  
  Holding age and region constant, Blueprinty users file \(\exp(0.19) \approx 1.21\) — about 21 % more patents than similar non-users.

* **Age follows an inverted-U.**  
  A positive linear term (≈ 0.12) and a small negative squared term (≈ –0.0025) imply the expected patent rate peaks at \(-\beta_{\text{age}} / (2\beta_{\text{age}^2}) \approx 25\) years of firm age, then declines gently.

* **Regions add little once we control for age and Blueprinty adoption.**  
  Northeast and Southwest carry small positive log effects (≈ 0.03–0.05), Northwest a small negative one (≈ –0.03).  
  All t-values sit near 1, so none is statistically different from the Midwest baseline at conventional levels.

* **Baseline level.**  
  The intercept (–0.15) translates to \(\exp(-0.15) ≈ 0.86\) patents over five years for a young Midwest firm that has not licensed Blueprinty.

Age dynamics and Blueprinty adoption drive most of the variation in patent output; regional location does not. For strategy, focus on signing up mid-career firms (~20–30 years old) — they sit at the productivity sweet spot and stand to gain a 20 % boost in patenting by adopting Blueprinty.


#### 3.4 Conclusion about the effect of Blueprinty's sofrware on patent success
Multiplicative rate ratios are handy, but managers usually ask, “So how many extra patents will Blueprinty actually help me win?”
We can answer that by predicting output for every firm twice:

1. **X₀** – same covariates as the data but force `customer = 0`.  
2. **X₁** – identical, except force `customer = 1`.

The average difference in the two prediction vectors gives a back-of-the-envelope “patents gained in five years” figure.

```{python}
#| label: customer-ate
#| echo: true
#| code-fold: true
import numpy as np
import pandas as pd

# make copies of the design matrix
X0 = X_mat.copy()
X1 = X_mat.copy()

# column index of the customer flag
cust_col = list(X_df.columns).index('customer')
X0[:, cust_col] = 0
X1[:, cust_col] = 1

# predictions under the two scenarios
y_pred_0 = np.exp(X0 @ beta_hat)          # expected patents if NOT a customer
y_pred_1 = np.exp(X1 @ beta_hat)          # expected patents if a customer

diff_vec = y_pred_1 - y_pred_0
ate_patents = diff_vec.mean()

print(f"Average treatment effect (Blueprinty vs none): "
      f"{ate_patents:.3f} extra patents per firm over 5 yrs")
```
In short, Blueprinty’s software delivers roughly one extra patent for every five firms that adopt it, a material gain for most R&D budgets.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. 

Key variables:

* `number_of_reviews` – our stand-in for bookings  
* Listing “age” (`days`)  
* Property traits: `room_type`, beds, baths, nightly `price`  
* Three 1-to-10 review scores: cleanliness, location, value  

```{python}
#| label: load-airbnb
#| echo: true
#| code-fold: true
#| results: hide   
import pandas as pd
import matplotlib.pyplot as plt

air = pd.read_csv('/Users/yoonjeong_park/mysite/_data/airbnb.csv')
```
Key fields are the unit’s host-tenure (`days`), property traits (`room_type`, beds, baths, price) and a reviews block that we’ll treat as a proxy for bookings.

### Preprocessing dataset

#### 1. Missing Value Check
Before plotting anything we need to know which columns are incomplete and how big the gaps are.

```{python}
#| echo: true
#| code-fold: true
#| label: missings

# percent missing per column
miss_pct = air.isna().mean().sort_values(ascending=False) * 100
miss_tbl = miss_pct.to_frame('pct_missing').round(1)
miss_tbl.head(10)        # show the ten worst offenders

# quick bar chart
miss_tbl.plot(kind='barh', figsize=(6,4), legend=False, color='#B19CD9')
plt.xlabel('% missing')
plt.title('Missing data by column')
plt.tight_layout()
plt.show()

```
The bar chart shows that missingness is concentrated in the three review-score columns—about 25 % of listings lack a cleanliness, location, or value score. Everything else (price, room type, bathrooms, etc.) is essentially complete (< 1 % missing).  

#### 2. Build a clean working dataset 
The missing-value scan told us that only the three review-score columns have sizable gaps. Everything else is virtually complete, so the simplest and safest move is to keep those score variables and drop the rows that are missing any of our modelling fields.

```{python}
#| label: make-clean
#| echo: true
#| code-fold: true
# columns we plan to use in the model
keep_cols = [
    'number_of_reviews',          
    'room_type',
    'bathrooms',
    'bedrooms',
    'price',
    'days',                      
    'review_scores_cleanliness',
    'review_scores_location',
    'review_scores_value',
]

air_cln = air.dropna(subset=keep_cols).copy()
print(f"Rows kept: {air_cln.shape[0]} of {air.shape[0]} "
      f"({air_cln.shape[0]/air.shape[0]:.0%})")
```
We now have a clean dataframe (air_cln) with all necessary variables present for roughly three-quarters of the original listings. That dataset is what we’ll use for plots, feature engineering, and the Poisson model in the next steps.

### Exploratory Data Analysis
We’ll start with a handful of simple plots:

1. Histogram of nightly price (spot outliers & decide on scaling).  
2. Histogram of `number_of_reviews` (our bookings proxy).  
3. Box-plots of reviews by `room_type` (entire / private / shared).  
4. Scatter of reviews vs. host tenure (`days`) to see if long-standing listings really do attract more business.

```{python}
#| label: eda-clean
#| echo: true
#| code-fold: true
import seaborn as sns

df = air_cln.copy()
# ── Trim crazy tails just for plotting ─────────────────────────
p99_price = np.percentile(df['price'], 99)
p99_days  = np.percentile(df['days'], 99)

df_plot = df[(df['price'] <= p99_price) & (df['days'] <= p99_days)].copy()

fig, ax = plt.subplots(2, 2, figsize=(10,6))

# 1. Price (under 99th-pct)
sns.histplot(df_plot['price'], bins=40, color='#6CA0DC', ax=ax[0,0])
ax[0,0].set_title('Nightly price ($)  – 99th-pct trimmed')

# 2. Reviews (log y-axis)
sns.histplot(df_plot['number_of_reviews'], bins=40,
             color='#B19CD9', ax=ax[0,1])
ax[0,1].set_yscale('log')
ax[0,1].set_title('Number of reviews  (log y)')

# 3. Violin plot by room type
sns.violinplot(x='room_type', y='number_of_reviews',
               data=df_plot, cut=0, inner='quartile',
               palette=['#6CA0DC','#B19CD9','#96d1b2'], ax=ax[1,0])
ax[1,0].set_yscale('log')
ax[1,0].set_title('Reviews by room type (log y)')

# 4. Reviews vs tenure (hexbin for density)
ax[1,1].clear()                                       # wipe the old hexbin

sample_idx = np.arange(0, df_plot.shape[0], 5)        # every 5th point for speed
ax[1,1].scatter(df_plot['days'].iloc[sample_idx] / 30,   # months
                df_plot['number_of_reviews'].iloc[sample_idx],
                s=12, alpha=0.08, color='#6CA0DC')

ax[1,1].set_xlabel('Months listed')
ax[1,1].set_ylabel('Reviews')
ax[1,1].set_yscale('log')
ax[1,1].set_title('Reviews vs. host tenure (log y)')

plt.tight_layout()
plt.show()
```
1. **Nightly price (histogram).**  
   - Most listings cluster between \$60 and \$200
   - The tail thins out quickly after \$300 even after trimming the extreme 1 %
   → A log or “price ÷ 100” scaling will keep the coefficient sizes readable.

2. **Number of reviews (histogram, log-y).**  
   The bar on the far left reminds us that many listings have just a handful of reviews, while a small elite racks up 200 +  
   → The long right tail is exactly why a Poisson/NB count model is appropriate.

3. **Reviews by room type (violin, log-y).**  
   *Entire homes* dominate the upper part of the distribution; *private* and especially *shared rooms* sit lower across the board  
   → We’ll keep dummy variables for these categories in the regression.

4. **Reviews vs. host tenure (scatter, log-y).**  
   A clear upward cloud: listings live longer tend to accumulate more reviews, but the slope flattens after about 24 months.  
   → We expect a positive, possibly diminishing-returns coefficient on
   `days`.

Together these patterns guide the model specification we build next.

### Poisson regression
Our plots suggest that review counts grow with listing age, fall with price, and differ by room type and review scores.  
We encode those insights in a log-linear Poisson model:


$$text{E[reviews]} \;=\; 
\exp\!\bigl(
\beta_0 
+ \beta_1 \,\text{months\_listed}
+ \beta_2 \,\text{price\_h} 
+ \beta_3 \,\text{bathrooms} 
+ \beta_4 \,\text{bedrooms}
+ \boldsymbol\beta_5^\top \!\text{scores}
+ \boldsymbol\beta_6^\top \!\text{room dummies}
\bigr)$$

```{python}
#| echo: true
#| code-fold: true

df = air_cln.copy()
df['price_h']      = df['price'] / 100
df['months_listed'] = df['days'] / 30

X_df = pd.concat([
    pd.Series(1.0, index=df.index, name='intercept'),
    df[['months_listed', 'price_h', 'bathrooms', 'bedrooms',
        'review_scores_cleanliness', 'review_scores_location',
        'review_scores_value']],
    pd.get_dummies(df['room_type'], drop_first=True)  # private / shared
], axis=1)

y_vec  = df['number_of_reviews'].values
X_mat  = X_df.values
```

```{python}
#| label: airbnb-model
#| echo: true
poiss_mod = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())
fit       = poiss_mod.fit()

coef_tbl = (
    pd.DataFrame({
        'coef':  fit.params,
        'std_err': fit.bse
    }, index=X_df.columns)
    .round(4)
)
coef_tbl
```
Because the model is log-linear, each coefficient shows the percentage change
in expected review for a one-unit bump in that variable (holding the others
fixed).  

Here’s what the signs and magnitudes tell us:

| Variable | β (≈) | What it means in practice |
|-----------|-------|---------------------------|
| **Months listed** | **+0.0015** | Each extra month on Airbnb lifts bookings by about 0.15 %. Over a year that compounds to ≈ 1.8 %. |
| **Price (+\$100)** | **-0.0037** | Raising the nightly rate \$100 trims bookings by ≈ 0.4 % — a very small price sensitivity in this market. |
| **Bathrooms** | **-0.11** | Surprisingly, adding a bathroom is linked to 10 % fewer reviews once price and bedrooms are held constant. Likely collinearity with luxury pricing. |
| **Bedrooms** | **+0.076** | An extra bedroom attracts ≈ 8 % more bookings. |
| **Cleanliness score (+1 pt)** | **+0.114** | A single-point bump (on the 1–10 scale) raises bookings by ≈ 12 %. Clean units sell. |
| **Location score (+1 pt)** | **-0.081** | Higher “location” scores correlate with ≈ 8 % fewer reviews. (Dense tourist zones may face tougher competition.) |
| **Value score (+1 pt)** | **-0.097** | A higher “value” score likewise shows a ≈ 9 % dip — indicating listings that over-deliver on value often compete on price and thus garner fewer stays overall. |
| **Private room vs. entire home** | **+0.012** | No real difference: booking volume is essentially the same. |
| **Shared room vs. entire home** | **-0.217** | Shared rooms draw ≈ 20 % fewer bookings than entire homes. |

### Conclusion
For New York City Airbnbs, bookings (reviews) are driven far more by perceived quality than by headline price.

* Add a bedroom → ~ 8 % more stays.  
* Keep the place spotless → each extra cleanliness star buys ~ 12 % more bookings.  
* Staying power counts, but only at the margin (≈ 2 % per year on the platform).  
* A \$100 price hike barely moves demand inside the common \$50–\$300 range.  
* Shared rooms remain a niche, drawing about 20 % fewer guests than entire homes; private rooms perform just as well as full units.

Therefore, hosts should focus effort (and budget) on cleanliness and clear value rather than aggressive price cuts; consider adding sleeping capacity before adding another bathroom; and remember that patience—accruing months and reviews—pays modest but steady dividends.





