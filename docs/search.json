[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yoonjeong Park",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYoonjeong Park\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYoonjeong Park\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large‑scale field experiment to see whether lowering the price of giving—by announcing a matching grant—boosts charitable donations. In August 2005 they mailed over 50,000 letters to prior supporters of a U.S. civil‑liberties nonprofit and randomly split the sample into a control group and a treatment group.\n\nControl letters looked exactly like the charity’s usual four‑page appeal.\nMatch letters were identical except for one added paragraph on page 2—and bold text on the reply card—announcing that an anonymous “leadership donor” would match any gift received.\n\nWithin the match arm, the researchers orthogonally randomized three design features:\n\n\n\n\n\n\n\nFactor\nLevels\n\n\n\n\nMatch ratio\n 1:1, 2:1, 3:1\n\n\nMaximum pledge\n $25 k, $50 k, $100 k, or unstated\n\n\nExample ask amount\ndonor’s highest previous gift, 1.25 × that gift, 1.50 × that gift\n\n\n\nThis yielded (3 = 36) distinct match treatments, all printed on otherwise identical stationery.\nThe two primary outcomes were (i) response rate — whether a donor gave within one month—and (ii) dollars contributed. Karlan & List find that any match increases the likelihood of giving by roughly 22 % and the revenue per letter by 19 %, yet higher match ratios (2:1 and 3:1) do not outperform the simple 1:1 offer. :contentReferenceoaicite:0​:contentReferenceoaicite:1\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large‑scale field experiment to see whether lowering the price of giving—by announcing a matching grant—boosts charitable donations. In August 2005 they mailed over 50,000 letters to prior supporters of a U.S. civil‑liberties nonprofit and randomly split the sample into a control group and a treatment group.\n\nControl letters looked exactly like the charity’s usual four‑page appeal.\nMatch letters were identical except for one added paragraph on page 2—and bold text on the reply card—announcing that an anonymous “leadership donor” would match any gift received.\n\nWithin the match arm, the researchers orthogonally randomized three design features:\n\n\n\n\n\n\n\nFactor\nLevels\n\n\n\n\nMatch ratio\n 1:1, 2:1, 3:1\n\n\nMaximum pledge\n $25 k, $50 k, $100 k, or unstated\n\n\nExample ask amount\ndonor’s highest previous gift, 1.25 × that gift, 1.50 × that gift\n\n\n\nThis yielded (3 = 36) distinct match treatments, all printed on otherwise identical stationery.\nThe two primary outcomes were (i) response rate — whether a donor gave within one month—and (ii) dollars contributed. Karlan & List find that any match increases the likelihood of giving by roughly 22 % and the revenue per letter by 19 %, yet higher match ratios (2:1 and 3:1) do not outperform the simple 1:1 offer. :contentReferenceoaicite:0​:contentReferenceoaicite:1\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis replication file contains 50,083 observations and 51 variables, with each row representing a single letter mailed in August 2005. Each record details the recipient’s random assignment, either a control appeal or one of 36 match treatments along with a rich set of pre‑treatment donor attributes. Two key outcomes are measured one month post‑mailing, whether a gift was made and the amount contributed, with no missing values in treatment or outcome fields. Overall missingness among covariates is below 5%, making the dataset effectively analysis ready from the start.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_stata(\"/Users/yoonjeong_park/mysite/_data/karlan_list_2007.dta\")\nprint(f\"{df.shape[0]:,} observations × {df.shape[1]} variables loaded.\")\n\n\n50,083 observations × 51 variables loaded.\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nTo confirm the randomization worked properly, I compared key baseline characteristics between the treatment and control groups. Because these metrics were recorded before any letters were sent, any clear differences would suggest an issue in how participants were randomly assigned.\nBaseline variables examined\n- hpa – Highest previous contribution (signals donor capacity)\n- mrm2 – Months since last donation (gauges recency)\n- freq – Number of prior gifts (tracks engagement)\n- female – Gender indicator (checks demographic balance)\nI ran two-sample t tests to see if the treatment and control group means differed. Formally, the t statistic measures how far apart the two group averages are, relative to the variability within each group:\n[ t = ]\nwhere (X_T) and (X_C) are the group means, (S_T^2) and (S_C^2) the sample variances, and (N_T), (N_C) the group sizes.\n\n\nShow code for t-tests\nimport numpy as np\nT, C = df[df.treatment==1], df[df.treatment==0]\n\ndef t_row(x, y):\n    n1,n2 = len(x),len(y)\n    m1,m2 = x.mean(),y.mean()\n    se = np.sqrt(x.var(ddof=1)/n1 + y.var(ddof=1)/n2)\n    t  = (m1-m2)/se\n    return m1, m2, m1-m2, t\n\nrows = [t_row(T[v], C[v]) for v in [\"hpa\",\"mrm2\",\"freq\",\"female\"]]\nttest = pd.DataFrame(\n    rows,\n    columns=[\"Mean T\",\"Mean C\",\"Diff\",\"t-stat\"],\n    index=[\"hpa\",\"mrm2\",\"freq\",\"female\"]\n).round(2)\n\nttest\n\n\n\n\n\n\nTwo-sample t-tests (Treatment − Control)\n\n\n\nMean T\nMean C\nDiff\nt-stat\n\n\n\n\nhpa\n59.60\n58.96\n0.64\n0.97\n\n\nmrm2\n13.01\n13.00\n0.01\n0.12\n\n\nfreq\n8.04\n8.05\n-0.01\n-0.11\n\n\nfemale\n0.28\n0.28\n-0.01\n-1.77\n\n\n\n\n\n\n\nNone of the four |t| values surpassed the usual 1.96 cutoff for a 5% significance level. In fact:\n\nhpa differs by only $0.64 on a $60 baseline—roughly a 1% gap, t = 0.97.\n\nmrm2 and freq are nearly identical (|t| &lt; 0.15).\n\nfemale shows the biggest difference, but |–1.77| is still below 1.96, so at an 8% significance level it’s borderline at best, and any apparent effect fades when adjusting for multiple comparisons.\n\nIn practical terms, treatment and control groups are alike in giving capacity, donation recency, engagement history, and gender composition. This aligns with Table 1 of Karlan & List (2007), confirming the randomization worked as intended. Since the baseline is balanced, any differences in donation behavior later on can be viewed as causal impacts of the match offer, rather than quirks of how participants were assigned.\nNext I replicate the same check with simple linear regression, which yields identical t statistics and reinforces this conclusion.\n\n\nShow code for OLS check\nimport statsmodels.formula.api as smf\ndef ols_coef(v):\n    m = smf.ols(f\"{v} ~ treatment\", data=df).fit()\n    return m.params[\"treatment\"], m.tvalues[\"treatment\"]\nols = pd.DataFrame(\n        [ols_coef(v) for v in [\"hpa\",\"mrm2\",\"freq\",\"female\"]],\n        columns=[\"Coef\",\"t-stat\"], index=[\"hpa\",\"mrm2\",\"freq\",\"female\"]).round(2)\nols\n\n\n\n\n\n\nOLS regression (coef on Treatment)\n\n\n\nCoef\nt-stat\n\n\n\n\nhpa\n0.64\n0.94\n\n\nmrm2\n0.01\n0.12\n\n\nfreq\n-0.01\n-0.11\n\n\nfemale\n-0.01\n-1.76\n\n\n\n\n\n\n\nThe manual two-sample t tests and our quick OLS regressions yield nearly identical t-statistics for each baseline variable (see the tables above). This aligns perfectly with theory: with a single 0/1 predictor, OLS replicates the standard two-sample t test.\nSince all |t| values remain well under the 1.96 threshold, we fail to reject the null hypothesis of equal means for hpa (giving capacity), mrm2 (donation recency), freq (engagement frequency), and female (gender). In simpler terms, the treatment and control groups start off the same.\nThis balance check validates the random assignment. It indicates that any differences observed later in donation behavior can be attributed to the matching-grant offer, rather than artifacts of who ended up in each group."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nThe first outcome measures whether each recipient donated at all. If matching grants effectively reduce the “price” of giving, the treatment group should naturally include more donors than the control group.\nWe begin by examining the share of recipients in each group who made a contribution, to see whether mentioning the matching grant drove more individuals to give.\n\n\n\n\n\n\n\n\n\nMerely mentioning that a leadership donor would match contributions prompted around one in five additional donors to respond. As the chart suggests, this difference is not just random fluctuation—it reflects a genuine behavioral response to the match offer.\nWhile the bar chart indicates that the match letter nudged more people to donate, a visual inspection alone doesn’t confirm statistical significance. To determine whether the 0.4 percentage-point increase is meaningful rather than chance, we:\n\nCompute a two-sample t statistic—the same technique used in our balance check, now applied to the binary outcomegave.\n\nRun the simplest regression, a single-predictor linear probability model (gave ~ treatment). By design, its t-statistic aligns with the manual test, but showing both methods assures transparency.\nCompare our findings to Table 2a, Panel A of Karlan & List (2007) to validate the replication.\n\nIf these tests yield small p-values—matching the original study’s results, we can be confident that simply mentioning a matching grant increases the fraction of donors.\n\n\nShow code for t-test & OLS\nimport numpy as np, pandas as pd, statsmodels.formula.api as smf, scipy.stats as st\n\n# Split groups\nT = df[df.treatment == 1][\"gave\"]\nC = df[df.treatment == 0][\"gave\"]\n\n# Manual two-sample t\ndiff  = T.mean() - C.mean()\nse    = np.sqrt(T.var(ddof=1)/len(T) + C.var(ddof=1)/len(C))\ntstat = diff / se\np_t   = 2 * (1 - st.t.cdf(abs(tstat), df=len(df)-2))\n\n# OLS regression\nols   = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Collect results\ntbl = pd.DataFrame({\n    \"Method\": [\"t-test\", \"OLS\"],\n    \"Diff (%-points)\": [diff*100, ols.params[\"treatment\"]*100],\n    \"t / t-stat\": [tstat, ols.tvalues[\"treatment\"]],\n    \"p-value\": [p_t, ols.pvalues[\"treatment\"]]\n}).round(3)\n\ntbl\n\n\n\n\n\n\nEffect of any match on donation probability\n\n\n\nMethod\nDiff (%-points)\nt / t-stat\np-value\n\n\n\n\n0\nt-test\n0.418\n3.209\n0.001\n\n\n1\nOLS\n0.418\n3.101\n0.002\n\n\n\n\n\n\n\nRecipients who knew their gifts would be matched were noticeably more inclined to donate compared to those who received the standard letter. This difference is unlikely to be due to chance: both a straightforward comparison and a quick regression reach the same conclusion. In other words, simply pointing out a matching opportunity nudges a significant number of recipients from “maybe later” to “yes, I’ll give,” in line with what Karlan & List originally reported.\nEconomists sometimes prefer a probit model for 0/1 outcomes because it naturally constrains predicted probabilities between 0 and 1. Applying the exact model from Table 3 (column 1) of Karlan & List—probit with a single treatment indicator—yields the same overall message:\n\n\nShow probit with marginal effect\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import Probit\n\n# Fit the probit\nprobit_mod = Probit(df[\"gave\"], sm.add_constant(df[\"treatment\"])).fit(disp=False)\n\n# Average marginal effect (overall)\name = probit_mod.get_margeff(at=\"overall\").summary_frame()\name.round(3)\n\n\n\n\n\n\nAverage marginal effect (matches Table 3, col 1 exactly)\n\n\n\ndy/dx\nStd. Err.\nz\nPr(&gt;|z|)\nConf. Int. Low\nCont. Int. Hi.\n\n\n\n\ntreatment\n0.004\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\nNo matter how we look at the data, the message is the same: donors are more likely to give when they know their gifts will be matched. A simple average comparison, a linear regression, and a probit model all reveal a small but meaningful lift—from just under two donors per hundred to just over two. This modest yet consistent effect aligns with Karlan & List’s original findings, showing that the mention of a matching grant successfully encourages a noticeable fraction of otherwise hesitant donors to contribute.\n\n\nDifferences between Match Rates\nNext, I explore how different match ratios—1:1, 2:1, and 3:1—affect donor response rates.\nSo far, I’ve grouped all match offers together. In reality, donors received three distinct “price discounts,” so the natural question is whether a higher match multiplier translates into higher participation.\n\n\n\n\n\n\nDonation share within each match ratio\n\n\n\nShare who gave\npercent\n\n\nratio\n\n\n\n\n\n\nControl\nNaN\nnan %\n\n\n1:1\n0.020749\n2.1 %\n\n\n2:1\n0.022633\n2.3 %\n\n\n3:1\n0.022733\n2.3 %\n\n\n\n\n\n\n\nPair-wise t-tests show no meaningful increase. Moving from a 1:1 to a 2:1 match raises the response rate by only 0.2 percentage points—far from the 1.96 threshold for conventional significance (t ≈ 1.0). The shift from 1:1 to 3:1 is similarly small and insignificant, and the comparison between 2:1 and 3:1 is essentially zero. These findings match Karlan & List’s own observation that “larger match ratios do not appear to matter much” (2007, p. 8). In other words, once donors know a match exists, raising the multiplier from 1:1 to 3:1 does not yield extra participation.\nTo verify these pairwise t-tests all at once, I ran a linear probability model that includes a separate indicator for each match ratio. Using 1:1 as the intuitive benchmark, the coefficients on 2:1 and 3:1 tells whether richer multipliers entice more donors.\n\n\nShow regression code\nimport pandas as pd, statsmodels.formula.api as smf\n\n# keep only letters that actually offered a match\nmt  = df[df.treatment == 1].copy()\n\n# make a clean 1/0 dummy for each ratio\nmt[\"ratio1\"] = (mt[\"ratio\"] == 1).astype(int)\n# ratio2 and ratio3 already exist as 1/0 flags in the data\n\n# OLS without an intercept so each coef = group mean\nreg = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=mt).fit()\nreg.summary2().tables[1].round(4)\n\n\n\n\n\n\nLinear probability model by match ratio (treatment arm only)\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nratio1\n0.0207\n0.0014\n14.9122\n0.0\n0.0180\n0.0235\n\n\nratio2\n0.0226\n0.0014\n16.2671\n0.0\n0.0199\n0.0254\n\n\nratio3\n0.0227\n0.0014\n16.3354\n0.0\n0.0200\n0.0255\n\n\n\n\n\n\n\n\nEach coefficient represents the donation rate for that match tier (since we dropped the intercept).\n\n1:1 yields roughly 2.07% of letters converting to donations.\n2:1 bumps that to 2.26%.\n3:1 lands at 2.27%.\n\nThe standard errors hover around 0.14 percentage points, so a 0.2 pp difference is well within one standard error. Formally, the t-stat for 2:1 minus 1:1 is about 0.6, and 3:1 vs. 2:1 is even smaller.\nThe very small p-values in the table refer to how precisely each mean is estimated (due to large N), not to any difference between the tiers themselves.\n\nHigher match ratios nudge the estimate upward by a fraction, but the change is too small relative to sampling variability to be considered reliable. In practice, a simple 1:1 match accounts for nearly the entire increase in participation. Upping the multiplier to 2:1 or 3:1 does not produce a statistically significant rise in donors—consistent with the authors’ remark on page 8.\nI quantified the gap by calculating the differences two ways: directly from the raw data and by subtracting the fitted coefficients from our model.\n\n\nShow code for response-rate differences\nimport numpy as np, pandas as pd\n\n# --- direct from data ---\nr1 = df[(df.treatment==1) & (df.ratio==1)][\"gave\"].mean()\nr2 = df[(df.treatment==1) & (df.ratio==2)][\"gave\"].mean()\nr3 = df[(df.treatment==1) & (df.ratio==3)][\"gave\"].mean()\n\ndirect = pd.Series({\n    \"2:1 − 1:1\": (r2 - r1) * 100,\n    \"3:1 − 2:1\": (r3 - r2) * 100\n})\n\n# --- from regression coefficients (reg from previous chunk) ---\ncoef = reg.params        # ratio1, ratio2, ratio3 means\nreg_diff = pd.Series({\n    \"2:1 − 1:1\": (coef[\"ratio2\"] - coef[\"ratio1\"]) * 100,\n    \"3:1 − 2:1\": (coef[\"ratio3\"] - coef[\"ratio2\"]) * 100\n})\n\npd.DataFrame({\n    \"Difference (pp) direct\": direct.round(2),\n    \"Difference (pp) from reg\": reg_diff.round(2)\n})\n\n\n\n\n\n\n\n\n\nDifference (pp) direct\nDifference (pp) from reg\n\n\n\n\n2:1 − 1:1\n0.19\n0.19\n\n\n3:1 − 2:1\n0.01\n0.01\n\n\n\n\n\n\n\nEven under close inspection, these gaps remain small:\n\nGoing from 1:1 to 2:1 delivers only a 0.19 percentage-point increase, which is well within the ±0.14 pp margin of error.\nShifting from 2:1 to 3:1 moves the needle by 0.01 pp—effectively zero.\n\nWhether measuring the differences directly or via regression coefficients, the conclusion is the same: richer match ratios do not generate a meaningful uptick in donors. The real boost comes from the presence of a match, not the size of it. For fundraisers, this suggests a straightforward 1:1 offer captures almost the full benefit without requiring the lead donor to fund a higher multiplier.\n\n\nSize of Charitable Contribution\nOur earlier analysis focused on whether a person decides to give at all. Now, we turn to whether the match letter also influences the total dollars raised.\nFirst, we treat every solicitation equally—including those that resulted in no donations. Under this approach, a match offer raises average revenue by about 15 cents per letter compared to the standard appeal. Although that’s roughly a 20% increase on an 81-cent baseline, the statistical significance is borderline, with the probability of seeing such a gap by chance hovering just above the 5% threshold. In other words, the rise in per-letter revenue seems mainly driven by converting more non-donors, yet the total lift remains modest and only marginally significant.\n\n\nShow code for unconditional amount\nimport pandas as pd, statsmodels.formula.api as smf, scipy.stats as st, numpy as np\n\n# raw means\nmean_control = df.loc[df.treatment == 0, \"amount\"].mean()\nmean_treat   = df.loc[df.treatment == 1, \"amount\"].mean()\n\n# t-test\nc = df[df.treatment == 0][\"amount\"]\nt = df[df.treatment == 1][\"amount\"]\nse = np.sqrt(c.var(ddof=1)/len(c) + t.var(ddof=1)/len(t))\ntstat = (t.mean() - c.mean()) / se\np_val = 2 * (1 - st.t.cdf(abs(tstat), df=len(df) - 2))\n\n# OLS check\nols_un = smf.ols(\"amount ~ treatment\", data=df).fit()\n\npd.DataFrame({\n    \"Mean ($) Control\": [mean_control.round(2)],\n    \"Mean ($) Treatment\": [mean_treat.round(2)],\n    \"Diff ($)\": [(mean_treat-mean_control).round(2)],\n    \"t-stat\": [round(tstat, 2)],\n    \"p-value\": [round(p_val, 3)]\n})\n\n\n\n\n\n\nAverage dollars per letter\n\n\n\nMean ($) Control\nMean ($) Treatment\nDiff ($)\nt-stat\np-value\n\n\n\n\n0\n0.81\n0.97\n0.15\n1.92\n0.055\n\n\n\n\n\n\n\nNext, we isolate people who already decided to donate to see if the match offer encouraged bigger gifts. Among those who opened their wallets, the match group’s average gift is around $43.9 versus $45.5 in the control—a difference of only $1.7, which is well within normal variation (its p-value is comfortably above common significance levels). Since this comparison involves a subset that the match treatment itself helped shape, it’s more descriptive than causal; the random assignment no longer ensures the donors in each group have similar traits. Overall, the clear story is that matching nudges more people to give but does not substantially increase average contributions among those who were already inclined to donate.\n\n\nShow code for conditional amount\ndonors = df[df.gave == 1].copy()\n\nmean_c = donors.loc[donors.treatment == 0, \"amount\"].mean()\nmean_t = donors.loc[donors.treatment == 1, \"amount\"].mean()\n\nols_con = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\npd.DataFrame({\n    \"Mean ($) Control\": [mean_c.round(2)],\n    \"Mean ($) Treatment\": [mean_t.round(2)],\n    \"Diff ($)\": [(mean_t-mean_c).round(2)],\n    \"p-value\": [round(ols_con.pvalues['treatment'], 3)]\n})\n\n\n\n\n\n\nAverage gift size among donors\n\n\n\nMean ($) Control\nMean ($) Treatment\nDiff ($)\np-value\n\n\n\n\n0\n45.540001\n43.869999\n-1.67\n0.561\n\n\n\n\n\n\n\n\n\n\n\n\nGift‐size distribution among donors\n\n\n\n\nTwo histograms drive home the same point. In both the treatment and control arms, most gifts fall below $50, with an occasional larger check beyond $200. The red dashed line in each histogram indicates the average gift, and these lines are nearly indistinguishable, reinforcing that the distribution of gift amounts barely changes when a match is mentioned. Put simply, matching grants boost participation rather than the average gift level.\nCombining these findings with the unconditional revenue result suggests that matching works by widening the donor base, not by persuading existing donors to give more. For fundraisers, this implies that the true value of a match lies in converting potential donors on the fence, while strategies aimed at increasing gift size will likely require different tactics."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nBelow is a plot illustrating how the sample average converges to the true treatment effect.\n\n\nShow simulation code\nimport numpy as np, matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# 100 000 control draws, 10 000 treatment draws\ncontrol_pool = np.random.binomial(1, 0.018, 100_000)\ntreat_draw   = np.random.binomial(1, 0.022, 10_000)\n\n# pick 10 000 control outcomes to pair with the 10 000 treatment draws\ncontrol_draw = control_pool[:10_000]            # simplest: first 10k\n\ndiff = treat_draw - control_draw                # 10 000 pair-wise gaps\ncum_avg = np.cumsum(diff) / np.arange(1, 10_001)\n\nplt.plot(cum_avg, linewidth=1)\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True effect 0.004\")\nplt.xlabel(\"Number of paired draws\")\nplt.ylabel(\"Cumulative average difference\")\nplt.title(\"Sample average converges to true treatment effect\")\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\nCumulative average of simulated donation-rate differences\n\n\n\n\nEarly on, the blue line fluctuates sharply because each new observation carries substantial weight when there are only a few paired draws. As the sample grows, these swings diminish, and the line settles into a narrow band around the dashed red line at 0.004. By around 5,000 pairs, the estimate already hovers within a few ten-thousandths of the target, and by 10,000 it rarely deviates. This pattern vividly demonstrates the Law of Large Numbers: when observations are independent, the cumulative average locks in on the actual population difference. In practical terms, it’s a clear reminder that larger samples generate more reliable estimates.\n\n\nCentral Limit Theorem\nTo visualize how sampling variation diminishes and how the distribution of results becomes more bell-shaped as n increases, I repeated the following experiment 1,000 times:\n\nDraw n observations from the control Bernoulli(0.018).\n\nDraw n observations from the treatment Bernoulli(0.022).\n\nRecord the difference in sample means.\n\nI ran this procedure for n = 50, 200, 500, and 1,000, then plot the distribution of the 1,000 simulated differences for each value of n.\n\n\nShow simulation code\nimport numpy as np, matplotlib.pyplot as plt\n\nnp.random.seed(42)\np_c, p_t = 0.018, 0.022\nsizes     = [50, 200, 500, 1000]\ndiffs_all = {}\n\nfor n in sizes:\n    control = np.random.binomial(1, p_c, (1000, n)).mean(axis=1)\n    treat   = np.random.binomial(1, p_t, (1000, n)).mean(axis=1)\n    diffs_all[n] = treat - control\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\naxes = axes.ravel()\n\nfor ax, n in zip(axes, sizes):\n    ax.hist(diffs_all[n], bins=30, color=\"#c29ff3\", alpha=0.9)\n    ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)      # reference at zero\n    ax.axvline(p_t - p_c, color=\"gold\", linestyle=\"-.\", linewidth=1)  # true effect 0.004\n    ax.set_title(f\"n = {n}\")\n    ax.set_xlabel(\"Difference in means\")\n    ax.set_ylabel(\"Count\")\n\nplt.tight_layout()\n\n\n\n\n\nSampling distribution of the treatment effect at four sample sizes\n\n\n\n\n\nn = 50\nThe histogram is wide and irregular, ranging roughly from –5% to +10%. The zero line (dashed) cuts through the center, indicating that a study with only 50 participants per group could easily produce an estimate near zero.\nn = 200\nThe distribution narrows into a more recognizable bell shape. Zero is still within the main cluster of data, though it’s no longer centered; chance alone can still yield an estimate near zero, but it’s less common.\nn = 500\nThe spread is noticeably tighter (most simulations land within ±2%). Zero now sits closer to the edge of the distribution’s bulk, so a study of this size would often reject the hypothesis of no effect.\nn = 1,000\nThe curve is sharply peaked and almost perfectly normal. Zero appears far in the left tail; only a small fraction of the 1,000 simulations drift that low. It becomes rare at this sample size to confuse the genuine effect with zero.\n\nTaken together, these snapshots illustrate the Central Limit Theore*: as sample size grows, the distribution of the differences becomes more bell-shaped, and its variance shrinks. Consequently, unlikely values like zero are pushed into the tails, making the true 0.4 percentage-point effect more evident in larger samples."
  },
  {
    "objectID": "projects/project1/Hw1_Code.html",
    "href": "projects/project1/Hw1_Code.html",
    "title": "Introduction",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large‑scale field experiment to see whether lowering the price of giving—by announcing a matching grant—boosts charitable donations. In August 2005 they mailed over 50,000 letters to prior supporters of a U.S. civil‑liberties nonprofit and randomly split the sample into a control group and a treatment group.\n\nControl letters looked exactly like the charity’s usual four‑page appeal.\nMatch letters were identical except for one added paragraph on page 2—and bold text on the reply card—announcing that an anonymous “leadership donor” would match any gift received.\n\nWithin the match arm, the researchers orthogonally randomized three design features:\n\n\n\n\n\n\n\nFactor\nLevels\n\n\n\n\nMatch ratio\n 1:1, 2:1, 3:1\n\n\nMaximum pledge\n $25 k, $50 k, $100 k, or unstated\n\n\nExample ask amount\ndonor’s highest previous gift, 1.25 × that gift, 1.50 × that gift\n\n\n\nThis yielded (3 = 36) distinct match treatments, all printed on otherwise identical stationery.\nThe two primary outcomes were (i) response rate — whether a donor gave within one month—and (ii) dollars contributed. Karlan & List find that any match increases the likelihood of giving by roughly 22 % and the revenue per letter by 19 %, yet higher match ratios (2:1 and 3:1) do not outperform the simple 1:1 offer. :contentReferenceoaicite:0​:contentReferenceoaicite:1\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#conclusion",
    "href": "projects/project1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis replication confirms Karlan & List’s core finding: simply telling potential donors their gift will be matched significantly increases participation, while moving from a 1:1 to a 2:1 or 3:1 match does not boost response further or affect average gift size. Randomization checks confirm the treatment and control groups are balanced, so the observed 0.4-percentage-point (≈22%) lift in response rate can be causally linked to the match offer. Probit, linear models, and pairwise t-tests all closely mirror the original paper’s results. The rise in revenue per letter stems mainly from converting more donors, rather than increasing average donation amounts—aligning with the authors’ “figures suggest” observation.\nSimulations of Bernoulli draws illustrate why these conclusions hold: the Law of Large Numbers drives the sample difference toward the true 0.004 effect, and the Central Limit Theorem shows that, with adequate sample size, zero falls to the tails of the sampling distribution. For practitioners, the lesson is straightforward: a one-for-one match captures nearly the full behavioral benefit at minimal cost, indicating that additional resources might be better spent recruiting new donors or targeting bigger gifts through other approaches."
  },
  {
    "objectID": "projects/project2/hw2_questions.html",
    "href": "projects/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty develops software that streamlines the blueprint drawings required for U.S. patent filings. The company’s marketing team suspects that firms using the tool see higher patent‑approval success than those that don’t. Ideally, we would compare each firm’s approval rate before and after adopting Blueprinty, but that historical view isn’t on file.\nInstead, we have a snapshot of 1,500 established engineering firms. For every company we know the number of patents awarded in the past five years, its region, age, and whether it licenses Blueprinty. By modeling these data we can isolate the impact of Blueprinty usage—controlling for basic firm attributes—and see whether the software really delivers a measurable edge in winning patents.\n\n\n\nBefore estimating any models, we first look at the raw Blueprinty dataset to understand how customer firms differ from non‑customers. This section walks through three quick diagnostics: (a) comparing patent output (b) checking whether observable firm traits—region (c) age—line up across the two groups. These checks set expectations for the Poisson regressions we will fit later.\n\n\n::: {#cell-Comparing Patent Output .cell execution_count=2}\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\ndf = pd.read_csv(\"/Users/yoonjeong_park/mysite/_data/blueprinty.csv\")   # adjust \n\n# corporate palette\nBLUE   = \"#6CA0DC\"   # customers\nPURPLE = \"#B19CD9\"   # non‑customers\n\n# ----- prepare counts -------------------------------------------------\nbins      = np.arange(df[\"patents\"].min(), df[\"patents\"].max() + 1)        # 0 … 16\ncust_cnt  = (df.loc[df[\"iscustomer\"] == 1, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\nnon_cnt   = (df.loc[df[\"iscustomer\"] == 0, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\n\n# ----- plot ------------------------------------------------------------\nx      = bins\nwidth  = 0.4                       # half‑bin width\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax.bar(x - width/2, cust_cnt.values,\n       width=width, color=BLUE,   label=\"Customer\")\nax.bar(x + width/2, non_cnt.values,\n       width=width, color=PURPLE, label=\"Non‑customer\")\n\n# nice, integer axes\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.set_xlabel(\"Number of patents\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Number of Patents\")\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n:::\nThe distribution of number of patents shows a clear picture:\n\nCustomers file a bit more. Most Blueprinty users sit around 4 patents and a few reach double digits.\n\nNon-customers stay low. Firms that don’t use the software pile up at 0–2 patents and rarely pass 6.\n\nStill plenty of overlap. The majority of all firms hold fewer than 6 patents, so the two groups aren’t miles apart.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nregion_counts = (\n    df\n    .groupby(['iscustomer', 'region'])\n    .size()\n    .unstack(fill_value=0)                    # rows → status, cols → region\n    .rename(index={0: 'Non-customer', 1: 'Customer'})\n)\n\n# ── make sure the rows are in the order we want ────────────────\nregion_counts_reordered = region_counts.loc[['Customer', 'Non-customer']]\n\n# ── grouped bar chart (not stacked) ────────────────────────────\nax = region_counts_reordered.T.plot(\n    kind='bar',\n    figsize=(7, 4),\n    width=0.75,                       # a touch wider than default\n    color=['#6CA0DC', '#B19CD9']      # Customer = blue, Non-customer = purple\n)\n\nplt.xlabel('Region')\nplt.ylabel('Number of firms')\nplt.title('Distribution of Regions by Customer Status')\nplt.xticks(rotation=0)\nplt.legend(title='Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bars plot say a lot about where Blueprinty’s users come from:\n\nNortheast is the hotspot. It hosts the biggest share of all firms and, notably, an even larger share of customers.\n\nEverywhere else skews non-customer. Midwest, Northwest, South, and Southwest each have many more non-customers than customers—sometimes five-to-one.\n\nRoom to grow. Those non-customer-heavy regions hint at obvious white-space for the sales team.\n\n\n\n\n\n\nAge distribution by customer status\n# ── Split ages by status ───────────────────────────────────────\nage_cust = df.loc[df.iscustomer == 1, 'age']\nage_non  = df.loc[df.iscustomer == 0, 'age']\n\n# ── Overlapping histograms ─────────────────────────────────────\nbins = np.arange(0, df['age'].max() + 5, 5)     # 5-year bins\n\nplt.figure(figsize=(7, 4))\nplt.hist(age_cust, bins=bins, alpha=0.7, color='#6CA0DC', label='Customer')\nplt.hist(age_non,  bins=bins, alpha=0.7, color='#B19CD9', label='Non-customer')\n\nplt.xlabel('Firm age (years)')\nplt.ylabel('Number of firms')\nplt.title('Age Distribution by Customer Status')\nplt.legend(title='Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nIf region separates customers from non-customers, age clearly does not:\n\nNearly identical curves. Both segments cluster in the 15-35-year window, with peaks around the mid-20s. You can barely tell one histogram from the other.\n\nFew outliers. Only a sprinkling of firms—customer or not—make it past 40 years. The extremes don’t tilt toward either side.\n\nFirm age doesn’t look like a deciding factor in adopting Blueprinty. When we estimate the Poisson model, we’ll control for age to be safe, but we don’t expect it to change the customer effect we saw in regions.\n\n\n\n\nSince our outcome, the count of patents in the last five years is a non-negative integer, the Poisson distribution is a natural starting point. We begin with the most stripped-down case: one common rate parameter, λ, for every firm and estimate it by Maximum Likelihood.\n\n\nFor a single firm\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\quad\\Longrightarrow\\quad\nf(Y_i\\mid\\lambda)\n=\\frac{e^{-\\lambda}\\,\\lambda^{Y_i}}{Y_i!}.\n\\]\nWith \\(n\\) independent firms the joint likelihood is the product of those densities:\n\\[\n\\mathcal{L}(\\lambda;\\mathbf y)\n=\\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n=e^{-n\\lambda}\\,\n  \\lambda^{\\sum_{i=1}^{n}y_i}\\Big/\\!\\prod_{i=1}^{n}y_i!.\n\\]\nThe log‑likelihood derived above is easy to translate into Python. All we need is (i) the rate parameter lmbda and (ii) an array of observed counts y.\n\nfrom scipy.special import gammaln   \n\ndef poisson_loglikelihood(lmbda: float, y: np.ndarray) -&gt; float:\n    y = np.asarray(y)\n    n = y.size\n    return -n * lmbda + y.sum() * np.log(lmbda) - gammaln(y + 1).sum()\n\n\n\n\nBefore trusting the optimiser, it’s good practice to see the curve we are maximising. Below I feed the observed patent counts into poisson_loglikelihood, evaluate it on a grid of candidate λ’s, and plot the log-likelihood. The peak should line up with the MLE we just computed.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- data & MLE from the previous step ---------------------------------\ny_data = df['patents'].values\nlambda_hat = y_data.mean()                   # analytic MLE\n\n# --- grid of λ values ---------------------------------------------------\nlam_grid = np.linspace(0.1, 8, 200)          # adjust max if needed\nloglik_vals = [poisson_loglikelihood(l, y_data) for l in lam_grid]\n\n# --- plot ---------------------------------------------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(lam_grid, loglik_vals, color='#6CA0DC', lw=2)\nplt.axvline(lambda_hat, color='#B19CD9', ls='--', lw=1.5,\n            label=f'λ̂ = {lambda_hat:.2f}')\nplt.xlabel('λ (candidate rate)')\nplt.ylabel('Log-likelihood  ℓ(λ)')\nplt.title('Log-likelihood profile for the simple Poisson model')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSingle, smooth peak. The log-likelihood rises sharply, tops out near λ̂ ≈ 3.7, and then drifts downward—exactly what we expect from a concave, well-behaved Poisson likelihood.\n\nMLE matches intuition. The peak sits almost exactly at the sample mean of the patent counts, foreshadowing the analytic result we’ll confirm next.\n\nNumerical stability. The curve is steep for very small λ but flattens around the optimum, so any reasonable optimiser will land in the same neighbourhood.\n\nNext we prove this formally: taking ∂ℓ/∂λ, setting it to zero, and showing the solution is the sample mean Ŷ = λ̂.\n\n\n\nStarting from the log-likelihood\n\\[[\n\\ell(\\lambda)\n   =-n\\lambda\n    +\\bigl(\\textstyle\\sum_{i=1}^{n}y_i\\bigr)\\log\\lambda\n    -\\sum_{i=1}^{n}\\log(y_i!)]\n\\]\ntake the first derivative:\n\\[[\\frac{\\partial \\ell}{\\partial\\lambda}\n   = -n + \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}]\\]\nSet the derivative to zero and solve for λ:\n\\[[ -n + \\frac{\\sum y_i}{\\lambda}=0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda\n  =\\frac{1}{n}\\sum_{i=1}^{n}y_i\n  =\\bar Y]\\]\nBecause the second derivative is negative, this critical point is indeed a maximum.\nWe can check this in Python as well.\n\n\nCode\ny_data = df['patents'].values\nlambda_mean = y_data.mean()          \n\nlambda_grid_peak = lambda_hat        \n\nprint(f\"Closed-form  λ̂ = {lambda_mean:.4f}\")\nprint(f\"Grid/optimiser λ̂ = {lambda_grid_peak:.4f}\")\n\n\nClosed-form  λ̂ = 3.6847\nGrid/optimiser λ̂ = 3.6847\n\n\nBoth paths land on the same estimate, confirming that the sample mean is the Maximum-Likelihood estimate for this simple Poisson model.\n\n\n\nTo close the loop we ask SciPy’s optimiser to find the λ that maximises our poisson_loglikelihood. (We tell it to minimise the negative log-likelihood.)\n\nfrom scipy.optimize import minimize_scalar\n\nneg_ll = lambda lmbda: -poisson_loglikelihood(lmbda, y_data)\nopt = minimize_scalar(neg_ll, bounds=(1e-6, 20), method='bounded')\n\nlambda_opt  = opt.x\nloglik_opt  = -opt.fun\n\nprint(f\"Optimiser  λ̂ = {lambda_opt:.4f}\")\nprint(f\"Log-lik at optimum = {loglik_opt:.2f}\")\nprint(f\"Closed-form λ̂     = {lambda_mean:.4f}\") \n\nOptimiser  λ̂ = 3.6847\nLog-lik at optimum = -3367.68\nClosed-form λ̂     = 3.6847\n\n\nThe optimiser lands at λ̂ = Ȳ and produces the same log-likelihood value we computed manually. This confirms that both analytical calculus and numerical search point to one maximum: the sample mean. With the basic Poisson model nailed down, we can now move on to richer specifications where λ varies with firm characteristics.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\nfrom scipy.special import gammaln   \n\ndef poisson_reg_loglik(beta: np.ndarray,\n                       y:   np.ndarray,\n                       X:   np.ndarray) -&gt; float:\n\n    y = np.asarray(y)\n    eta = X @ beta              \n    lam = np.exp(eta)              \n    \n    return (y * eta - lam - gammaln(y + 1)).sum()\n\nThe new function is nothing more than the “one-λ” log-likelihood from Section 2, applied row-wise with firm-specific rates:\n\nη = X @ β is the linear score for each firm.\n\nλ = exp(η) enforces positivity and makes each coefficient a multiplicative effect:\na one-unit bump in a covariate scales expected patents by exp(β_j).\n\nSumming the Poisson kernel across firms gives the total log-likelihood.\n\n\n\n\nWe now assemble the design matrix X (intercept + age + age² + region dummies + customer flag) and let SciPy’s BFGS optimiser hunt for the β̂ that maximises poisson_reg_loglik.\nBFGS also returns an approximation to the inverse Hessian, which we use as an estimate of the coefficient variance–covariance matrix.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# ── log-likelihood, gradient, Hessian ─────────────────────────\ndef pois_reg_components(beta, y, X):\n    \"\"\"Return ℓ, ∇ℓ, and −H (Fisher info) for Poisson regression.\"\"\"\n    eta = X @ beta\n    lam = np.exp(np.clip(eta, -20, 20))        # overflow guard\n    ll  = (y * eta - lam - gammaln(y + 1)).sum()\n    grad = X.T @ (y - lam)\n    hess = -(X.T * lam) @ X                    # negative definite\n    return ll, grad, hess\n\ndef neg_ll_grad_hess(beta, y, X):\n    ll, grad, hess = pois_reg_components(beta, y, X)\n    return -ll, -grad, -hess                   # minimise −ℓ\n\n# ── design matrix (intercept + covariates) ────────────────────\nX_df = pd.DataFrame({\n    'intercept' : 1.0,\n    'age'       : df['age'],\n    'age_sq'    : df['age']**2,\n    'customer'  : df['iscustomer']\n})\nX_df = pd.concat([X_df,\n                  pd.get_dummies(df['region'], drop_first=True)], axis=1)\n\ny_vec, X_mat = df['patents'].values, X_df.values\np            = X_mat.shape[1]\n\n# ── optimise using Newton–CG with analytic pieces ─────────────\ninit_beta = np.zeros(p)\n\nopt = minimize(lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[0],\n               init_beta,\n               method='Newton-CG',\n               jac=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[1],\n               hess=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[2],\n               options={'xtol': 1e-8, 'disp': False})\n\nif not opt.success:\n    raise RuntimeError(opt.message)\n\nbeta_hat = opt.x\n# covariance ≈ (−H)^−1 at optimum\n_, _, hess_opt = pois_reg_components(beta_hat, y_vec, X_mat)\ncov_hat = np.linalg.inv(-hess_opt)\nse_hat  = np.sqrt(np.diag(cov_hat))\n\nresults = (\n    pd.DataFrame({'coef': beta_hat, 'std_err': se_hat},\n                 index=X_df.columns)\n    .round(4)\n)\nresults\n\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n-0.1483\n0.1785\n\n\nage\n0.1225\n0.0135\n\n\nage_sq\n-0.0025\n0.0003\n\n\ncustomer\n0.1870\n0.0309\n\n\nNortheast\n0.0255\n0.0433\n\n\nNorthwest\n-0.0330\n0.0536\n\n\nSouth\n0.0295\n0.0526\n\n\nSouthwest\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe table below translates directly into multiplicative rate effects because our link is log-linear ( λ = exp (Xβ) ).\n\nIntercept (-0.1483).\nBaseline firm — Midwest, non-customer, age = 0 — is expected to file exp(−0.15) ≈ 0.86 patents in five years.\nAge terms (0.1225 and –0.0025).\nA one-year increase raises the log rate by 0.1225 but the negative squared term drags it down, creating an inverted-U.\nCustomer flag (0.1870).\nHolding everything else constant, Blueprinty clients file exp(0.187) ≈ 1.21 times — 21 % more** — patents than non-clients.\nRegion effects (relative to Midwest).\nAll point estimates are small (±3 %) and their standard errors are of similar magnitude → no strong regional tilt once we control for age and customer status.\n\nMost coefficients have |t| &gt; 2 (β / SE), so the age dynamics and the customer bump are statistically meaningful, whereas regional differences are not.\n\n\n\nTo be sure our hand-rolled MLE matches a mature library, we refit the exact same design matrix with statsmodels’ Poisson GLM and compare coefficients.\n\n\nCode\nimport statsmodels.api as sm\n\nglm_poiss = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = glm_poiss.fit()\n\nglm_table = (\n    pd.DataFrame({\n        'coef_glm' : fit.params,\n        'se_glm'   : fit.bse,\n        'coef_mle' : beta_hat,     \n        'se_mle'   : se_hat\n    }, index=X_df.columns)\n    .round(4)\n)\nglm_table\n\n\n\n\n\n\n\n\n\ncoef_glm\nse_glm\ncoef_mle\nse_mle\n\n\n\n\nintercept\n-0.5089\n0.1832\n-0.1483\n0.1785\n\n\nage\n0.1486\n0.0139\n0.1225\n0.0135\n\n\nage_sq\n-0.0030\n0.0003\n-0.0025\n0.0003\n\n\ncustomer\n0.2076\n0.0309\n0.1870\n0.0309\n\n\nNortheast\n0.0292\n0.0436\n0.0255\n0.0433\n\n\nNorthwest\n-0.0176\n0.0538\n-0.0330\n0.0536\n\n\nSouth\n0.0566\n0.0527\n0.0295\n0.0526\n\n\nSouthwest\n0.0506\n0.0472\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe GLM check confirms our custom MLE, so we can interpret the estimates with confidence.\n\nBlueprinty pays off.\nThe customer coefficient is 0.19 (SE ≈ 0.03).\nHolding age and region constant, Blueprinty users file ((0.19) ) — about 21 % more patents than similar non-users.\nAge follows an inverted-U.\nA positive linear term (≈ 0.12) and a small negative squared term (≈ –0.0025) imply the expected patent rate peaks at (-{} / (2{^2}) ) years of firm age, then declines gently.\nRegions add little once we control for age and Blueprinty adoption.\nNortheast and Southwest carry small positive log effects (≈ 0.03–0.05), Northwest a small negative one (≈ –0.03).\nAll t-values sit near 1, so none is statistically different from the Midwest baseline at conventional levels.\nBaseline level.\nThe intercept (–0.15) translates to ((-0.15) ≈ 0.86) patents over five years for a young Midwest firm that has not licensed Blueprinty.\n\nAge dynamics and Blueprinty adoption drive most of the variation in patent output; regional location does not. For strategy, focus on signing up mid-career firms (~20–30 years old) — they sit at the productivity sweet spot and stand to gain a 20 % boost in patenting by adopting Blueprinty.\n\n\n\nMultiplicative rate ratios are handy, but managers usually ask, “So how many extra patents will Blueprinty actually help me win?” We can answer that by predicting output for every firm twice:\n\nX₀ – same covariates as the data but force customer = 0.\n\nX₁ – identical, except force customer = 1.\n\nThe average difference in the two prediction vectors gives a back-of-the-envelope “patents gained in five years” figure.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# make copies of the design matrix\nX0 = X_mat.copy()\nX1 = X_mat.copy()\n\n# column index of the customer flag\ncust_col = list(X_df.columns).index('customer')\nX0[:, cust_col] = 0\nX1[:, cust_col] = 1\n\n# predictions under the two scenarios\ny_pred_0 = np.exp(X0 @ beta_hat)          # expected patents if NOT a customer\ny_pred_1 = np.exp(X1 @ beta_hat)          # expected patents if a customer\n\ndiff_vec = y_pred_1 - y_pred_0\nate_patents = diff_vec.mean()\n\nprint(f\"Average treatment effect (Blueprinty vs none): \"\n      f\"{ate_patents:.3f} extra patents per firm over 5 yrs\")\n\n\nAverage treatment effect (Blueprinty vs none): 0.713 extra patents per firm over 5 yrs\n\n\nIn short, Blueprinty’s software delivers roughly one extra patent for every five firms that adopt it, a material gain for most R&D budgets."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty develops software that streamlines the blueprint drawings required for U.S. patent filings. The company’s marketing team suspects that firms using the tool see higher patent‑approval success than those that don’t. Ideally, we would compare each firm’s approval rate before and after adopting Blueprinty, but that historical view isn’t on file.\nInstead, we have a snapshot of 1,500 established engineering firms. For every company we know the number of patents awarded in the past five years, its region, age, and whether it licenses Blueprinty. By modeling these data we can isolate the impact of Blueprinty usage—controlling for basic firm attributes—and see whether the software really delivers a measurable edge in winning patents.\n\n\n\nBefore estimating any models, we first look at the raw Blueprinty dataset to understand how customer firms differ from non‑customers. This section walks through three quick diagnostics: (a) comparing patent output (b) checking whether observable firm traits—region (c) age—line up across the two groups. These checks set expectations for the Poisson regressions we will fit later.\n\n\n::: {#cell-Comparing Patent Output .cell execution_count=2}\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\ndf = pd.read_csv(\"/Users/yoonjeong_park/mysite/_data/blueprinty.csv\")   # adjust \n\n# corporate palette\nBLUE   = \"#6CA0DC\"   # customers\nPURPLE = \"#B19CD9\"   # non‑customers\n\n# ----- prepare counts -------------------------------------------------\nbins      = np.arange(df[\"patents\"].min(), df[\"patents\"].max() + 1)        # 0 … 16\ncust_cnt  = (df.loc[df[\"iscustomer\"] == 1, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\nnon_cnt   = (df.loc[df[\"iscustomer\"] == 0, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\n\n# ----- plot ------------------------------------------------------------\nx      = bins\nwidth  = 0.4                       # half‑bin width\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax.bar(x - width/2, cust_cnt.values,\n       width=width, color=BLUE,   label=\"Customer\")\nax.bar(x + width/2, non_cnt.values,\n       width=width, color=PURPLE, label=\"Non‑customer\")\n\n# nice, integer axes\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.set_xlabel(\"Number of patents\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Number of Patents\")\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n:::\nThe distribution of number of patents shows a clear picture:\n\nCustomers file a bit more. Most Blueprinty users sit around 4 patents and a few reach double digits.\n\nNon-customers stay low. Firms that don’t use the software pile up at 0–2 patents and rarely pass 6.\n\nStill plenty of overlap. The majority of all firms hold fewer than 6 patents, so the two groups aren’t miles apart.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nregion_counts = (\n    df\n    .groupby(['iscustomer', 'region'])\n    .size()\n    .unstack(fill_value=0)                    # rows → status, cols → region\n    .rename(index={0: 'Non-customer', 1: 'Customer'})\n)\n\n# ── make sure the rows are in the order we want ────────────────\nregion_counts_reordered = region_counts.loc[['Customer', 'Non-customer']]\n\n# ── grouped bar chart (not stacked) ────────────────────────────\nax = region_counts_reordered.T.plot(\n    kind='bar',\n    figsize=(7, 4),\n    width=0.75,                       # a touch wider than default\n    color=['#6CA0DC', '#B19CD9']      # Customer = blue, Non-customer = purple\n)\n\nplt.xlabel('Region')\nplt.ylabel('Number of firms')\nplt.title('Distribution of Regions by Customer Status')\nplt.xticks(rotation=0)\nplt.legend(title='Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bars plot say a lot about where Blueprinty’s users come from:\n\nNortheast is the hotspot. It hosts the biggest share of all firms and, notably, an even larger share of customers.\n\nEverywhere else skews non-customer. Midwest, Northwest, South, and Southwest each have many more non-customers than customers—sometimes five-to-one.\n\nRoom to grow. Those non-customer-heavy regions hint at obvious white-space for the sales team.\n\n\n\n\n\n\nAge distribution by customer status\n# ── Split ages by status ───────────────────────────────────────\nage_cust = df.loc[df.iscustomer == 1, 'age']\nage_non  = df.loc[df.iscustomer == 0, 'age']\n\n# ── Overlapping histograms ─────────────────────────────────────\nbins = np.arange(0, df['age'].max() + 5, 5)     # 5-year bins\n\nplt.figure(figsize=(7, 4))\nplt.hist(age_cust, bins=bins, alpha=0.7, color='#6CA0DC', label='Customer')\nplt.hist(age_non,  bins=bins, alpha=0.7, color='#B19CD9', label='Non-customer')\n\nplt.xlabel('Firm age (years)')\nplt.ylabel('Number of firms')\nplt.title('Age Distribution by Customer Status')\nplt.legend(title='Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nIf region separates customers from non-customers, age clearly does not:\n\nNearly identical curves. Both segments cluster in the 15-35-year window, with peaks around the mid-20s. You can barely tell one histogram from the other.\n\nFew outliers. Only a sprinkling of firms—customer or not—make it past 40 years. The extremes don’t tilt toward either side.\n\nFirm age doesn’t look like a deciding factor in adopting Blueprinty. When we estimate the Poisson model, we’ll control for age to be safe, but we don’t expect it to change the customer effect we saw in regions.\n\n\n\n\nSince our outcome, the count of patents in the last five years is a non-negative integer, the Poisson distribution is a natural starting point. We begin with the most stripped-down case: one common rate parameter, λ, for every firm and estimate it by Maximum Likelihood.\n\n\nFor a single firm\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\quad\\Longrightarrow\\quad\nf(Y_i\\mid\\lambda)\n=\\frac{e^{-\\lambda}\\,\\lambda^{Y_i}}{Y_i!}.\n\\]\nWith \\(n\\) independent firms the joint likelihood is the product of those densities:\n\\[\n\\mathcal{L}(\\lambda;\\mathbf y)\n=\\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n=e^{-n\\lambda}\\,\n  \\lambda^{\\sum_{i=1}^{n}y_i}\\Big/\\!\\prod_{i=1}^{n}y_i!.\n\\]\nThe log‑likelihood derived above is easy to translate into Python. All we need is (i) the rate parameter lmbda and (ii) an array of observed counts y.\n\nfrom scipy.special import gammaln   \n\ndef poisson_loglikelihood(lmbda: float, y: np.ndarray) -&gt; float:\n    y = np.asarray(y)\n    n = y.size\n    return -n * lmbda + y.sum() * np.log(lmbda) - gammaln(y + 1).sum()\n\n\n\n\nBefore trusting the optimiser, it’s good practice to see the curve we are maximising. Below I feed the observed patent counts into poisson_loglikelihood, evaluate it on a grid of candidate λ’s, and plot the log-likelihood. The peak should line up with the MLE we just computed.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- data & MLE from the previous step ---------------------------------\ny_data = df['patents'].values\nlambda_hat = y_data.mean()                   # analytic MLE\n\n# --- grid of λ values ---------------------------------------------------\nlam_grid = np.linspace(0.1, 8, 200)          # adjust max if needed\nloglik_vals = [poisson_loglikelihood(l, y_data) for l in lam_grid]\n\n# --- plot ---------------------------------------------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(lam_grid, loglik_vals, color='#6CA0DC', lw=2)\nplt.axvline(lambda_hat, color='#B19CD9', ls='--', lw=1.5,\n            label=f'λ̂ = {lambda_hat:.2f}')\nplt.xlabel('λ (candidate rate)')\nplt.ylabel('Log-likelihood  ℓ(λ)')\nplt.title('Log-likelihood profile for the simple Poisson model')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSingle, smooth peak. The log-likelihood rises sharply, tops out near λ̂ ≈ 3.7, and then drifts downward—exactly what we expect from a concave, well-behaved Poisson likelihood.\n\nMLE matches intuition. The peak sits almost exactly at the sample mean of the patent counts, foreshadowing the analytic result we’ll confirm next.\n\nNumerical stability. The curve is steep for very small λ but flattens around the optimum, so any reasonable optimiser will land in the same neighbourhood.\n\nNext we prove this formally: taking ∂ℓ/∂λ, setting it to zero, and showing the solution is the sample mean Ŷ = λ̂.\n\n\n\nStarting from the log-likelihood\n\\[[\n\\ell(\\lambda)\n   =-n\\lambda\n    +\\bigl(\\textstyle\\sum_{i=1}^{n}y_i\\bigr)\\log\\lambda\n    -\\sum_{i=1}^{n}\\log(y_i!)]\n\\]\ntake the first derivative:\n\\[[\\frac{\\partial \\ell}{\\partial\\lambda}\n   = -n + \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}]\\]\nSet the derivative to zero and solve for λ:\n\\[[ -n + \\frac{\\sum y_i}{\\lambda}=0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda\n  =\\frac{1}{n}\\sum_{i=1}^{n}y_i\n  =\\bar Y]\\]\nBecause the second derivative is negative, this critical point is indeed a maximum.\nWe can check this in Python as well.\n\n\nCode\ny_data = df['patents'].values\nlambda_mean = y_data.mean()          \n\nlambda_grid_peak = lambda_hat        \n\nprint(f\"Closed-form  λ̂ = {lambda_mean:.4f}\")\nprint(f\"Grid/optimiser λ̂ = {lambda_grid_peak:.4f}\")\n\n\nClosed-form  λ̂ = 3.6847\nGrid/optimiser λ̂ = 3.6847\n\n\nBoth paths land on the same estimate, confirming that the sample mean is the Maximum-Likelihood estimate for this simple Poisson model.\n\n\n\nTo close the loop we ask SciPy’s optimiser to find the λ that maximises our poisson_loglikelihood. (We tell it to minimise the negative log-likelihood.)\n\nfrom scipy.optimize import minimize_scalar\n\nneg_ll = lambda lmbda: -poisson_loglikelihood(lmbda, y_data)\nopt = minimize_scalar(neg_ll, bounds=(1e-6, 20), method='bounded')\n\nlambda_opt  = opt.x\nloglik_opt  = -opt.fun\n\nprint(f\"Optimiser  λ̂ = {lambda_opt:.4f}\")\nprint(f\"Log-lik at optimum = {loglik_opt:.2f}\")\nprint(f\"Closed-form λ̂     = {lambda_mean:.4f}\") \n\nOptimiser  λ̂ = 3.6847\nLog-lik at optimum = -3367.68\nClosed-form λ̂     = 3.6847\n\n\nThe optimiser lands at λ̂ = Ȳ and produces the same log-likelihood value we computed manually. This confirms that both analytical calculus and numerical search point to one maximum: the sample mean. With the basic Poisson model nailed down, we can now move on to richer specifications where λ varies with firm characteristics.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\nfrom scipy.special import gammaln   \n\ndef poisson_reg_loglik(beta: np.ndarray,\n                       y:   np.ndarray,\n                       X:   np.ndarray) -&gt; float:\n\n    y = np.asarray(y)\n    eta = X @ beta              \n    lam = np.exp(eta)              \n    \n    return (y * eta - lam - gammaln(y + 1)).sum()\n\nThe new function is nothing more than the “one-λ” log-likelihood from Section 2, applied row-wise with firm-specific rates:\n\nη = X @ β is the linear score for each firm.\n\nλ = exp(η) enforces positivity and makes each coefficient a multiplicative effect:\na one-unit bump in a covariate scales expected patents by exp(β_j).\n\nSumming the Poisson kernel across firms gives the total log-likelihood.\n\n\n\n\nWe now assemble the design matrix X (intercept + age + age² + region dummies + customer flag) and let SciPy’s BFGS optimiser hunt for the β̂ that maximises poisson_reg_loglik.\nBFGS also returns an approximation to the inverse Hessian, which we use as an estimate of the coefficient variance–covariance matrix.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# ── log-likelihood, gradient, Hessian ─────────────────────────\ndef pois_reg_components(beta, y, X):\n    \"\"\"Return ℓ, ∇ℓ, and −H (Fisher info) for Poisson regression.\"\"\"\n    eta = X @ beta\n    lam = np.exp(np.clip(eta, -20, 20))        # overflow guard\n    ll  = (y * eta - lam - gammaln(y + 1)).sum()\n    grad = X.T @ (y - lam)\n    hess = -(X.T * lam) @ X                    # negative definite\n    return ll, grad, hess\n\ndef neg_ll_grad_hess(beta, y, X):\n    ll, grad, hess = pois_reg_components(beta, y, X)\n    return -ll, -grad, -hess                   # minimise −ℓ\n\n# ── design matrix (intercept + covariates) ────────────────────\nX_df = pd.DataFrame({\n    'intercept' : 1.0,\n    'age'       : df['age'],\n    'age_sq'    : df['age']**2,\n    'customer'  : df['iscustomer']\n})\nX_df = pd.concat([X_df,\n                  pd.get_dummies(df['region'], drop_first=True)], axis=1)\n\ny_vec, X_mat = df['patents'].values, X_df.values\np            = X_mat.shape[1]\n\n# ── optimise using Newton–CG with analytic pieces ─────────────\ninit_beta = np.zeros(p)\n\nopt = minimize(lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[0],\n               init_beta,\n               method='Newton-CG',\n               jac=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[1],\n               hess=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[2],\n               options={'xtol': 1e-8, 'disp': False})\n\nif not opt.success:\n    raise RuntimeError(opt.message)\n\nbeta_hat = opt.x\n# covariance ≈ (−H)^−1 at optimum\n_, _, hess_opt = pois_reg_components(beta_hat, y_vec, X_mat)\ncov_hat = np.linalg.inv(-hess_opt)\nse_hat  = np.sqrt(np.diag(cov_hat))\n\nresults = (\n    pd.DataFrame({'coef': beta_hat, 'std_err': se_hat},\n                 index=X_df.columns)\n    .round(4)\n)\nresults\n\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n-0.1483\n0.1785\n\n\nage\n0.1225\n0.0135\n\n\nage_sq\n-0.0025\n0.0003\n\n\ncustomer\n0.1870\n0.0309\n\n\nNortheast\n0.0255\n0.0433\n\n\nNorthwest\n-0.0330\n0.0536\n\n\nSouth\n0.0295\n0.0526\n\n\nSouthwest\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe table below translates directly into multiplicative rate effects because our link is log-linear ( λ = exp (Xβ) ).\n\nIntercept (-0.1483).\nBaseline firm — Midwest, non-customer, age = 0 — is expected to file exp(−0.15) ≈ 0.86 patents in five years.\nAge terms (0.1225 and –0.0025).\nA one-year increase raises the log rate by 0.1225 but the negative squared term drags it down, creating an inverted-U.\nCustomer flag (0.1870).\nHolding everything else constant, Blueprinty clients file exp(0.187) ≈ 1.21 times — 21 % more** — patents than non-clients.\nRegion effects (relative to Midwest).\nAll point estimates are small (±3 %) and their standard errors are of similar magnitude → no strong regional tilt once we control for age and customer status.\n\nMost coefficients have |t| &gt; 2 (β / SE), so the age dynamics and the customer bump are statistically meaningful, whereas regional differences are not.\n\n\n\nTo be sure our hand-rolled MLE matches a mature library, we refit the exact same design matrix with statsmodels’ Poisson GLM and compare coefficients.\n\n\nCode\nimport statsmodels.api as sm\n\nglm_poiss = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = glm_poiss.fit()\n\nglm_table = (\n    pd.DataFrame({\n        'coef_glm' : fit.params,\n        'se_glm'   : fit.bse,\n        'coef_mle' : beta_hat,     \n        'se_mle'   : se_hat\n    }, index=X_df.columns)\n    .round(4)\n)\nglm_table\n\n\n\n\n\n\n\n\n\ncoef_glm\nse_glm\ncoef_mle\nse_mle\n\n\n\n\nintercept\n-0.5089\n0.1832\n-0.1483\n0.1785\n\n\nage\n0.1486\n0.0139\n0.1225\n0.0135\n\n\nage_sq\n-0.0030\n0.0003\n-0.0025\n0.0003\n\n\ncustomer\n0.2076\n0.0309\n0.1870\n0.0309\n\n\nNortheast\n0.0292\n0.0436\n0.0255\n0.0433\n\n\nNorthwest\n-0.0176\n0.0538\n-0.0330\n0.0536\n\n\nSouth\n0.0566\n0.0527\n0.0295\n0.0526\n\n\nSouthwest\n0.0506\n0.0472\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe GLM check confirms our custom MLE, so we can interpret the estimates with confidence.\n\nBlueprinty pays off.\nThe customer coefficient is 0.19 (SE ≈ 0.03).\nHolding age and region constant, Blueprinty users file ((0.19) ) — about 21 % more patents than similar non-users.\nAge follows an inverted-U.\nA positive linear term (≈ 0.12) and a small negative squared term (≈ –0.0025) imply the expected patent rate peaks at (-{} / (2{^2}) ) years of firm age, then declines gently.\nRegions add little once we control for age and Blueprinty adoption.\nNortheast and Southwest carry small positive log effects (≈ 0.03–0.05), Northwest a small negative one (≈ –0.03).\nAll t-values sit near 1, so none is statistically different from the Midwest baseline at conventional levels.\nBaseline level.\nThe intercept (–0.15) translates to ((-0.15) ≈ 0.86) patents over five years for a young Midwest firm that has not licensed Blueprinty.\n\nAge dynamics and Blueprinty adoption drive most of the variation in patent output; regional location does not. For strategy, focus on signing up mid-career firms (~20–30 years old) — they sit at the productivity sweet spot and stand to gain a 20 % boost in patenting by adopting Blueprinty.\n\n\n\nMultiplicative rate ratios are handy, but managers usually ask, “So how many extra patents will Blueprinty actually help me win?” We can answer that by predicting output for every firm twice:\n\nX₀ – same covariates as the data but force customer = 0.\n\nX₁ – identical, except force customer = 1.\n\nThe average difference in the two prediction vectors gives a back-of-the-envelope “patents gained in five years” figure.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# make copies of the design matrix\nX0 = X_mat.copy()\nX1 = X_mat.copy()\n\n# column index of the customer flag\ncust_col = list(X_df.columns).index('customer')\nX0[:, cust_col] = 0\nX1[:, cust_col] = 1\n\n# predictions under the two scenarios\ny_pred_0 = np.exp(X0 @ beta_hat)          # expected patents if NOT a customer\ny_pred_1 = np.exp(X1 @ beta_hat)          # expected patents if a customer\n\ndiff_vec = y_pred_1 - y_pred_0\nate_patents = diff_vec.mean()\n\nprint(f\"Average treatment effect (Blueprinty vs none): \"\n      f\"{ate_patents:.3f} extra patents per firm over 5 yrs\")\n\n\nAverage treatment effect (Blueprinty vs none): 0.713 extra patents per firm over 5 yrs\n\n\nIn short, Blueprinty’s software delivers roughly one extra patent for every five firms that adopt it, a material gain for most R&D budgets."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#airbnb-case-study",
    "href": "projects/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.\nKey variables:\n\nnumber_of_reviews – our stand-in for bookings\n\nListing “age” (days)\n\nProperty traits: room_type, beds, baths, nightly price\n\nThree 1-to-10 review scores: cleanliness, location, value\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nair = pd.read_csv('/Users/yoonjeong_park/mysite/_data/airbnb.csv')\n\n\nKey fields are the unit’s host-tenure (days), property traits (room_type, beds, baths, price) and a reviews block that we’ll treat as a proxy for bookings.\n\n\nPreprocessing dataset\n\n1. Missing Value Check\nBefore plotting anything we need to know which columns are incomplete and how big the gaps are.\n\n\nCode\n# percent missing per column\nmiss_pct = air.isna().mean().sort_values(ascending=False) * 100\nmiss_tbl = miss_pct.to_frame('pct_missing').round(1)\nmiss_tbl.head(10)        # show the ten worst offenders\n\n# quick bar chart\nmiss_tbl.plot(kind='barh', figsize=(6,4), legend=False, color='#B19CD9')\nplt.xlabel('% missing')\nplt.title('Missing data by column')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart shows that missingness is concentrated in the three review-score columns—about 25 % of listings lack a cleanliness, location, or value score. Everything else (price, room type, bathrooms, etc.) is essentially complete (&lt; 1 % missing).\n\n\n2. Build a clean working dataset\nThe missing-value scan told us that only the three review-score columns have sizable gaps. Everything else is virtually complete, so the simplest and safest move is to keep those score variables and drop the rows that are missing any of our modelling fields.\n\n\nCode\n# columns we plan to use in the model\nkeep_cols = [\n    'number_of_reviews',          \n    'room_type',\n    'bathrooms',\n    'bedrooms',\n    'price',\n    'days',                      \n    'review_scores_cleanliness',\n    'review_scores_location',\n    'review_scores_value',\n]\n\nair_cln = air.dropna(subset=keep_cols).copy()\nprint(f\"Rows kept: {air_cln.shape[0]} of {air.shape[0]} \"\n      f\"({air_cln.shape[0]/air.shape[0]:.0%})\")\n\n\nRows kept: 30160 of 40628 (74%)\n\n\nWe now have a clean dataframe (air_cln) with all necessary variables present for roughly three-quarters of the original listings. That dataset is what we’ll use for plots, feature engineering, and the Poisson model in the next steps.\n\n\n\nExploratory Data Analysis\nWe’ll start with a handful of simple plots:\n\nHistogram of nightly price (spot outliers & decide on scaling).\n\nHistogram of number_of_reviews (our bookings proxy).\n\nBox-plots of reviews by room_type (entire / private / shared).\n\nScatter of reviews vs. host tenure (days) to see if long-standing listings really do attract more business.\n\n\n\nCode\nimport seaborn as sns\n\ndf = air_cln.copy()\n# ── Trim crazy tails just for plotting ─────────────────────────\np99_price = np.percentile(df['price'], 99)\np99_days  = np.percentile(df['days'], 99)\n\ndf_plot = df[(df['price'] &lt;= p99_price) & (df['days'] &lt;= p99_days)].copy()\n\nfig, ax = plt.subplots(2, 2, figsize=(10,6))\n\n# 1. Price (under 99th-pct)\nsns.histplot(df_plot['price'], bins=40, color='#6CA0DC', ax=ax[0,0])\nax[0,0].set_title('Nightly price ($)  – 99th-pct trimmed')\n\n# 2. Reviews (log y-axis)\nsns.histplot(df_plot['number_of_reviews'], bins=40,\n             color='#B19CD9', ax=ax[0,1])\nax[0,1].set_yscale('log')\nax[0,1].set_title('Number of reviews  (log y)')\n\n# 3. Violin plot by room type\nsns.violinplot(x='room_type', y='number_of_reviews',\n               data=df_plot, cut=0, inner='quartile',\n               palette=['#6CA0DC','#B19CD9','#96d1b2'], ax=ax[1,0])\nax[1,0].set_yscale('log')\nax[1,0].set_title('Reviews by room type (log y)')\n\n# 4. Reviews vs tenure (hexbin for density)\nax[1,1].clear()                                       # wipe the old hexbin\n\nsample_idx = np.arange(0, df_plot.shape[0], 5)        # every 5th point for speed\nax[1,1].scatter(df_plot['days'].iloc[sample_idx] / 30,   # months\n                df_plot['number_of_reviews'].iloc[sample_idx],\n                s=12, alpha=0.08, color='#6CA0DC')\n\nax[1,1].set_xlabel('Months listed')\nax[1,1].set_ylabel('Reviews')\nax[1,1].set_yscale('log')\nax[1,1].set_title('Reviews vs. host tenure (log y)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNightly price (histogram).\n\nMost listings cluster between $60 and $200\nThe tail thins out quickly after $300 even after trimming the extreme 1 % → A log or “price ÷ 100” scaling will keep the coefficient sizes readable.\n\nNumber of reviews (histogram, log-y).\nThe bar on the far left reminds us that many listings have just a handful of reviews, while a small elite racks up 200 +\n→ The long right tail is exactly why a Poisson/NB count model is appropriate.\nReviews by room type (violin, log-y).\nEntire homes dominate the upper part of the distribution; private and especially shared rooms sit lower across the board\n→ We’ll keep dummy variables for these categories in the regression.\nReviews vs. host tenure (scatter, log-y).\nA clear upward cloud: listings live longer tend to accumulate more reviews, but the slope flattens after about 24 months.\n→ We expect a positive, possibly diminishing-returns coefficient on days.\n\nTogether these patterns guide the model specification we build next.\n\n\nPoisson regression\nOur plots suggest that review counts grow with listing age, fall with price, and differ by room type and review scores.\nWe encode those insights in a log-linear Poisson model:\n\\[[\n\\text{E[reviews]} \\;=\\;\n\\exp\\!\\bigl(\n\\beta_0\n+ \\beta_1 \\,\\text{months\\_listed}\n+ \\beta_2 \\,\\text{price\\_h}\n+ \\beta_3 \\,\\text{bathrooms}\n+ \\beta_4 \\,\\text{bedrooms}\n+ \\boldsymbol\\beta_5^\\top \\!\\text{scores}\n+ \\boldsymbol\\beta_6^\\top \\!\\text{room dummies}\n\\bigr)\n\\]\\]\n\n\nCode\ndf = air_cln.copy()\ndf['price_h']      = df['price'] / 100\ndf['months_listed'] = df['days'] / 30\n\nX_df = pd.concat([\n    pd.Series(1.0, index=df.index, name='intercept'),\n    df[['months_listed', 'price_h', 'bathrooms', 'bedrooms',\n        'review_scores_cleanliness', 'review_scores_location',\n        'review_scores_value']],\n    pd.get_dummies(df['room_type'], drop_first=True)  # private / shared\n], axis=1)\n\ny_vec  = df['number_of_reviews'].values\nX_mat  = X_df.values\n\n\n\npoiss_mod = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = poiss_mod.fit()\n\ncoef_tbl = (\n    pd.DataFrame({\n        'coef':  fit.params,\n        'std_err': fit.bse\n    }, index=X_df.columns)\n    .round(4)\n)\ncoef_tbl\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n3.6458\n0.0160\n\n\nmonths_listed\n0.0015\n0.0000\n\n\nprice_h\n-0.0037\n0.0009\n\n\nbathrooms\n-0.1105\n0.0038\n\n\nbedrooms\n0.0756\n0.0020\n\n\nreview_scores_cleanliness\n0.1138\n0.0015\n\n\nreview_scores_location\n-0.0809\n0.0016\n\n\nreview_scores_value\n-0.0971\n0.0018\n\n\nPrivate room\n0.0121\n0.0027\n\n\nShared room\n-0.2172\n0.0086\n\n\n\n\n\n\n\nBecause the model is log-linear, each coefficient shows the percentage change in expected review for a one-unit bump in that variable (holding the others fixed).\nHere’s what the signs and magnitudes tell us:\n\n\n\n\n\n\n\n\nVariable\nβ (≈)\nWhat it means in practice\n\n\n\n\nMonths listed\n+0.0015\nEach extra month on Airbnb lifts bookings by about 0.15 %. Over a year that compounds to ≈ 1.8 %.\n\n\nPrice (+$100)\n-0.0037\nRaising the nightly rate $100 trims bookings by ≈ 0.4 % — a very small price sensitivity in this market.\n\n\nBathrooms\n-0.11\nSurprisingly, adding a bathroom is linked to 10 % fewer reviews once price and bedrooms are held constant. Likely collinearity with luxury pricing.\n\n\nBedrooms\n+0.076\nAn extra bedroom attracts ≈ 8 % more bookings.\n\n\nCleanliness score (+1 pt)\n+0.114\nA single-point bump (on the 1–10 scale) raises bookings by ≈ 12 %. Clean units sell.\n\n\nLocation score (+1 pt)\n-0.081\nHigher “location” scores correlate with ≈ 8 % fewer reviews. (Dense tourist zones may face tougher competition.)\n\n\nValue score (+1 pt)\n-0.097\nA higher “value” score likewise shows a ≈ 9 % dip — indicating listings that over-deliver on value often compete on price and thus garner fewer stays overall.\n\n\nPrivate room vs. entire home\n+0.012\nNo real difference: booking volume is essentially the same.\n\n\nShared room vs. entire home\n-0.217\nShared rooms draw ≈ 20 % fewer bookings than entire homes.\n\n\n\n\n\nConclusion\nFor New York City Airbnbs, bookings (reviews) are driven far more by perceived quality than by headline price.\n\nAdd a bedroom → ~ 8 % more stays.\n\nKeep the place spotless → each extra cleanliness star buys ~ 12 % more bookings.\n\nStaying power counts, but only at the margin (≈ 2 % per year on the platform).\n\nA $100 price hike barely moves demand inside the common $50–$300 range.\n\nShared rooms remain a niche, drawing about 20 % fewer guests than entire homes; private rooms perform just as well as full units.\n\nTherefore, hosts should focus effort (and budget) on cleanliness and clear value rather than aggressive price cuts; consider adding sleeping capacity before adding another bathroom; and remember that patience—accruing months and reviews—pays modest but steady dividends."
  },
  {
    "objectID": "projects/project3/hw2_questions (2).html",
    "href": "projects/project3/hw2_questions (2).html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty develops software that streamlines the blueprint drawings required for U.S. patent filings. The company’s marketing team suspects that firms using the tool see higher patent‑approval success than those that don’t. Ideally, we would compare each firm’s approval rate before and after adopting Blueprinty, but that historical view isn’t on file.\nInstead, we have a snapshot of 1,500 established engineering firms. For every company we know the number of patents awarded in the past five years, its region, age, and whether it licenses Blueprinty. By modeling these data we can isolate the impact of Blueprinty usage—controlling for basic firm attributes—and see whether the software really delivers a measurable edge in winning patents.\n\n\n\nBefore estimating any models, we first look at the raw Blueprinty dataset to understand how customer firms differ from non‑customers. This section walks through three quick diagnostics: (a) comparing patent output (b) checking whether observable firm traits—region (c) age—line up across the two groups. These checks set expectations for the Poisson regressions we will fit later.\n\n\n::: {#cell-Comparing Patent Output .cell execution_count=2}\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\ndf = pd.read_csv(\"/Users/yoonjeong_park/mysite/_data/blueprinty.csv\")   # adjust \n\n# corporate palette\nBLUE   = \"#6CA0DC\"   # customers\nPURPLE = \"#B19CD9\"   # non‑customers\n\n# ----- prepare counts -------------------------------------------------\nbins      = np.arange(df[\"patents\"].min(), df[\"patents\"].max() + 1)        # 0 … 16\ncust_cnt  = (df.loc[df[\"iscustomer\"] == 1, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\nnon_cnt   = (df.loc[df[\"iscustomer\"] == 0, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\n\n# ----- plot ------------------------------------------------------------\nx      = bins\nwidth  = 0.4                       # half‑bin width\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax.bar(x - width/2, cust_cnt.values,\n       width=width, color=BLUE,   label=\"Customer\")\nax.bar(x + width/2, non_cnt.values,\n       width=width, color=PURPLE, label=\"Non‑customer\")\n\n# nice, integer axes\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.set_xlabel(\"Number of patents\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Number of Patents\")\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n:::\nThe distribution of number of patents shows a clear picture:\n\nCustomers file a bit more. Most Blueprinty users sit around 4 patents and a few reach double digits.\n\nNon-customers stay low. Firms that don’t use the software pile up at 0–2 patents and rarely pass 6.\n\nStill plenty of overlap. The majority of all firms hold fewer than 6 patents, so the two groups aren’t miles apart.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nregion_counts = (\n    df\n    .groupby(['iscustomer', 'region'])\n    .size()\n    .unstack(fill_value=0)                    # rows → status, cols → region\n    .rename(index={0: 'Non-customer', 1: 'Customer'})\n)\n\n# ── make sure the rows are in the order we want ────────────────\nregion_counts_reordered = region_counts.loc[['Customer', 'Non-customer']]\n\n# ── grouped bar chart (not stacked) ────────────────────────────\nax = region_counts_reordered.T.plot(\n    kind='bar',\n    figsize=(7, 4),\n    width=0.75,                       # a touch wider than default\n    color=['#6CA0DC', '#B19CD9']      # Customer = blue, Non-customer = purple\n)\n\nplt.xlabel('Region')\nplt.ylabel('Number of firms')\nplt.title('Distribution of Regions by Customer Status')\nplt.xticks(rotation=0)\nplt.legend(title='Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bars plot say a lot about where Blueprinty’s users come from:\n\nNortheast is the hotspot. It hosts the biggest share of all firms and, notably, an even larger share of customers.\n\nEverywhere else skews non-customer. Midwest, Northwest, South, and Southwest each have many more non-customers than customers—sometimes five-to-one.\n\nRoom to grow. Those non-customer-heavy regions hint at obvious white-space for the sales team.\n\n\n\n\n\n\nAge distribution by customer status\n# ── Split ages by status ───────────────────────────────────────\nage_cust = df.loc[df.iscustomer == 1, 'age']\nage_non  = df.loc[df.iscustomer == 0, 'age']\n\n# ── Overlapping histograms ─────────────────────────────────────\nbins = np.arange(0, df['age'].max() + 5, 5)     # 5-year bins\n\nplt.figure(figsize=(7, 4))\nplt.hist(age_cust, bins=bins, alpha=0.7, color='#6CA0DC', label='Customer')\nplt.hist(age_non,  bins=bins, alpha=0.7, color='#B19CD9', label='Non-customer')\n\nplt.xlabel('Firm age (years)')\nplt.ylabel('Number of firms')\nplt.title('Age Distribution by Customer Status')\nplt.legend(title='Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nIf region separates customers from non-customers, age clearly does not:\n\nNearly identical curves. Both segments cluster in the 15-35-year window, with peaks around the mid-20s. You can barely tell one histogram from the other.\n\nFew outliers. Only a sprinkling of firms—customer or not—make it past 40 years. The extremes don’t tilt toward either side.\n\nFirm age doesn’t look like a deciding factor in adopting Blueprinty. When we estimate the Poisson model, we’ll control for age to be safe, but we don’t expect it to change the customer effect we saw in regions.\n\n\n\n\nSince our outcome, the count of patents in the last five years is a non-negative integer, the Poisson distribution is a natural starting point. We begin with the most stripped-down case: one common rate parameter, λ, for every firm and estimate it by Maximum Likelihood.\n\n\nFor a single firm\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\quad\\Longrightarrow\\quad\nf(Y_i\\mid\\lambda)\n=\\frac{e^{-\\lambda}\\,\\lambda^{Y_i}}{Y_i!}.\n\\]\nWith \\(n\\) independent firms the joint likelihood is the product of those densities:\n\\[\n\\mathcal{L}(\\lambda;\\mathbf y)\n=\\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n=e^{-n\\lambda}\\,\n  \\lambda^{\\sum_{i=1}^{n}y_i}\\Big/\\!\\prod_{i=1}^{n}y_i!.\n\\]\nThe log‑likelihood derived above is easy to translate into Python. All we need is (i) the rate parameter lmbda and (ii) an array of observed counts y.\n\nfrom scipy.special import gammaln   \n\ndef poisson_loglikelihood(lmbda: float, y: np.ndarray) -&gt; float:\n    y = np.asarray(y)\n    n = y.size\n    return -n * lmbda + y.sum() * np.log(lmbda) - gammaln(y + 1).sum()\n\n\n\n\nBefore trusting the optimiser, it’s good practice to see the curve we are maximising. Below I feed the observed patent counts into poisson_loglikelihood, evaluate it on a grid of candidate λ’s, and plot the log-likelihood. The peak should line up with the MLE we just computed.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- data & MLE from the previous step ---------------------------------\ny_data = df['patents'].values\nlambda_hat = y_data.mean()                   # analytic MLE\n\n# --- grid of λ values ---------------------------------------------------\nlam_grid = np.linspace(0.1, 8, 200)          # adjust max if needed\nloglik_vals = [poisson_loglikelihood(l, y_data) for l in lam_grid]\n\n# --- plot ---------------------------------------------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(lam_grid, loglik_vals, color='#6CA0DC', lw=2)\nplt.axvline(lambda_hat, color='#B19CD9', ls='--', lw=1.5,\n            label=f'λ̂ = {lambda_hat:.2f}')\nplt.xlabel('λ (candidate rate)')\nplt.ylabel('Log-likelihood  ℓ(λ)')\nplt.title('Log-likelihood profile for the simple Poisson model')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSingle, smooth peak. The log-likelihood rises sharply, tops out near λ̂ ≈ 3.7, and then drifts downward—exactly what we expect from a concave, well-behaved Poisson likelihood.\n\nMLE matches intuition. The peak sits almost exactly at the sample mean of the patent counts, foreshadowing the analytic result we’ll confirm next.\n\nNumerical stability. The curve is steep for very small λ but flattens around the optimum, so any reasonable optimiser will land in the same neighbourhood.\n\nNext we prove this formally: taking ∂ℓ/∂λ, setting it to zero, and showing the solution is the sample mean Ŷ = λ̂.\n\n\n\nStarting from the log-likelihood\n\\[[\n\\ell(\\lambda)\n   =-n\\lambda\n    +\\bigl(\\textstyle\\sum_{i=1}^{n}y_i\\bigr)\\log\\lambda\n    -\\sum_{i=1}^{n}\\log(y_i!)]\n\\]\ntake the first derivative:\n\\[[\\frac{\\partial \\ell}{\\partial\\lambda}\n   = -n + \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}]\\]\nSet the derivative to zero and solve for λ:\n\\[[ -n + \\frac{\\sum y_i}{\\lambda}=0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda\n  =\\frac{1}{n}\\sum_{i=1}^{n}y_i\n  =\\bar Y]\\]\nBecause the second derivative is negative, this critical point is indeed a maximum.\nWe can check this in Python as well.\n\n\nCode\ny_data = df['patents'].values\nlambda_mean = y_data.mean()          \n\nlambda_grid_peak = lambda_hat        \n\nprint(f\"Closed-form  λ̂ = {lambda_mean:.4f}\")\nprint(f\"Grid/optimiser λ̂ = {lambda_grid_peak:.4f}\")\n\n\nClosed-form  λ̂ = 3.6847\nGrid/optimiser λ̂ = 3.6847\n\n\nBoth paths land on the same estimate, confirming that the sample mean is the Maximum-Likelihood estimate for this simple Poisson model.\n\n\n\nTo close the loop we ask SciPy’s optimiser to find the λ that maximises our poisson_loglikelihood. (We tell it to minimise the negative log-likelihood.)\n\nfrom scipy.optimize import minimize_scalar\n\nneg_ll = lambda lmbda: -poisson_loglikelihood(lmbda, y_data)\nopt = minimize_scalar(neg_ll, bounds=(1e-6, 20), method='bounded')\n\nlambda_opt  = opt.x\nloglik_opt  = -opt.fun\n\nprint(f\"Optimiser  λ̂ = {lambda_opt:.4f}\")\nprint(f\"Log-lik at optimum = {loglik_opt:.2f}\")\nprint(f\"Closed-form λ̂     = {lambda_mean:.4f}\") \n\nOptimiser  λ̂ = 3.6847\nLog-lik at optimum = -3367.68\nClosed-form λ̂     = 3.6847\n\n\nThe optimiser lands at λ̂ = Ȳ and produces the same log-likelihood value we computed manually. This confirms that both analytical calculus and numerical search point to one maximum: the sample mean. With the basic Poisson model nailed down, we can now move on to richer specifications where λ varies with firm characteristics.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\nfrom scipy.special import gammaln   \n\ndef poisson_reg_loglik(beta: np.ndarray,\n                       y:   np.ndarray,\n                       X:   np.ndarray) -&gt; float:\n\n    y = np.asarray(y)\n    eta = X @ beta              \n    lam = np.exp(eta)              \n    \n    return (y * eta - lam - gammaln(y + 1)).sum()\n\nThe new function is nothing more than the “one-λ” log-likelihood from Section 2, applied row-wise with firm-specific rates:\n\nη = X @ β is the linear score for each firm.\n\nλ = exp(η) enforces positivity and makes each coefficient a multiplicative effect:\na one-unit bump in a covariate scales expected patents by exp(β_j).\n\nSumming the Poisson kernel across firms gives the total log-likelihood.\n\n\n\n\nWe now assemble the design matrix X (intercept + age + age² + region dummies + customer flag) and let SciPy’s BFGS optimiser hunt for the β̂ that maximises poisson_reg_loglik.\nBFGS also returns an approximation to the inverse Hessian, which we use as an estimate of the coefficient variance–covariance matrix.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# ── log-likelihood, gradient, Hessian ─────────────────────────\ndef pois_reg_components(beta, y, X):\n    \"\"\"Return ℓ, ∇ℓ, and −H (Fisher info) for Poisson regression.\"\"\"\n    eta = X @ beta\n    lam = np.exp(np.clip(eta, -20, 20))        # overflow guard\n    ll  = (y * eta - lam - gammaln(y + 1)).sum()\n    grad = X.T @ (y - lam)\n    hess = -(X.T * lam) @ X                    # negative definite\n    return ll, grad, hess\n\ndef neg_ll_grad_hess(beta, y, X):\n    ll, grad, hess = pois_reg_components(beta, y, X)\n    return -ll, -grad, -hess                   # minimise −ℓ\n\n# ── design matrix (intercept + covariates) ────────────────────\nX_df = pd.DataFrame({\n    'intercept' : 1.0,\n    'age'       : df['age'],\n    'age_sq'    : df['age']**2,\n    'customer'  : df['iscustomer']\n})\nX_df = pd.concat([X_df,\n                  pd.get_dummies(df['region'], drop_first=True)], axis=1)\n\ny_vec, X_mat = df['patents'].values, X_df.values\np            = X_mat.shape[1]\n\n# ── optimise using Newton–CG with analytic pieces ─────────────\ninit_beta = np.zeros(p)\n\nopt = minimize(lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[0],\n               init_beta,\n               method='Newton-CG',\n               jac=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[1],\n               hess=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[2],\n               options={'xtol': 1e-8, 'disp': False})\n\nif not opt.success:\n    raise RuntimeError(opt.message)\n\nbeta_hat = opt.x\n# covariance ≈ (−H)^−1 at optimum\n_, _, hess_opt = pois_reg_components(beta_hat, y_vec, X_mat)\ncov_hat = np.linalg.inv(-hess_opt)\nse_hat  = np.sqrt(np.diag(cov_hat))\n\nresults = (\n    pd.DataFrame({'coef': beta_hat, 'std_err': se_hat},\n                 index=X_df.columns)\n    .round(4)\n)\nresults\n\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n-0.1483\n0.1785\n\n\nage\n0.1225\n0.0135\n\n\nage_sq\n-0.0025\n0.0003\n\n\ncustomer\n0.1870\n0.0309\n\n\nNortheast\n0.0255\n0.0433\n\n\nNorthwest\n-0.0330\n0.0536\n\n\nSouth\n0.0295\n0.0526\n\n\nSouthwest\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe table below translates directly into multiplicative rate effects because our link is log-linear ( λ = exp (Xβ) ).\n\nIntercept (-0.1483).\nBaseline firm — Midwest, non-customer, age = 0 — is expected to file exp(−0.15) ≈ 0.86 patents in five years.\nAge terms (0.1225 and –0.0025).\nA one-year increase raises the log rate by 0.1225 but the negative squared term drags it down, creating an inverted-U.\nCustomer flag (0.1870).\nHolding everything else constant, Blueprinty clients file exp(0.187) ≈ 1.21 times — 21 % more** — patents than non-clients.\nRegion effects (relative to Midwest).\nAll point estimates are small (±3 %) and their standard errors are of similar magnitude → no strong regional tilt once we control for age and customer status.\n\nMost coefficients have |t| &gt; 2 (β / SE), so the age dynamics and the customer bump are statistically meaningful, whereas regional differences are not.\n\n\n\nTo be sure our hand-rolled MLE matches a mature library, we refit the exact same design matrix with statsmodels’ Poisson GLM and compare coefficients.\n\n\nCode\nimport statsmodels.api as sm\n\nglm_poiss = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = glm_poiss.fit()\n\nglm_table = (\n    pd.DataFrame({\n        'coef_glm' : fit.params,\n        'se_glm'   : fit.bse,\n        'coef_mle' : beta_hat,     \n        'se_mle'   : se_hat\n    }, index=X_df.columns)\n    .round(4)\n)\nglm_table\n\n\n\n\n\n\n\n\n\ncoef_glm\nse_glm\ncoef_mle\nse_mle\n\n\n\n\nintercept\n-0.5089\n0.1832\n-0.1483\n0.1785\n\n\nage\n0.1486\n0.0139\n0.1225\n0.0135\n\n\nage_sq\n-0.0030\n0.0003\n-0.0025\n0.0003\n\n\ncustomer\n0.2076\n0.0309\n0.1870\n0.0309\n\n\nNortheast\n0.0292\n0.0436\n0.0255\n0.0433\n\n\nNorthwest\n-0.0176\n0.0538\n-0.0330\n0.0536\n\n\nSouth\n0.0566\n0.0527\n0.0295\n0.0526\n\n\nSouthwest\n0.0506\n0.0472\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe GLM check confirms our custom MLE, so we can interpret the estimates with confidence.\n\nBlueprinty pays off.\nThe customer coefficient is 0.19 (SE ≈ 0.03).\nHolding age and region constant, Blueprinty users file ((0.19) ) — about 21 % more patents than similar non-users.\nAge follows an inverted-U.\nA positive linear term (≈ 0.12) and a small negative squared term (≈ –0.0025) imply the expected patent rate peaks at (-{} / (2{^2}) ) years of firm age, then declines gently.\nRegions add little once we control for age and Blueprinty adoption.\nNortheast and Southwest carry small positive log effects (≈ 0.03–0.05), Northwest a small negative one (≈ –0.03).\nAll t-values sit near 1, so none is statistically different from the Midwest baseline at conventional levels.\nBaseline level.\nThe intercept (–0.15) translates to ((-0.15) ≈ 0.86) patents over five years for a young Midwest firm that has not licensed Blueprinty.\n\nAge dynamics and Blueprinty adoption drive most of the variation in patent output; regional location does not. For strategy, focus on signing up mid-career firms (~20–30 years old) — they sit at the productivity sweet spot and stand to gain a 20 % boost in patenting by adopting Blueprinty.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\n\nMultiplicative rate ratios are handy, but managers usually ask, “So how many extra patents will Blueprinty actually help me win?” We can answer that by predicting output for every firm twice:\n\nX₀ – same covariates as the data but force customer = 0.\n\nX₁ – identical, except force customer = 1.\n\nThe average difference in the two prediction vectors gives a back-of-the-envelope “patents gained in five years” figure.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# make copies of the design matrix\nX0 = X_mat.copy()\nX1 = X_mat.copy()\n\n# column index of the customer flag\ncust_col = list(X_df.columns).index('customer')\nX0[:, cust_col] = 0\nX1[:, cust_col] = 1\n\n# predictions under the two scenarios\ny_pred_0 = np.exp(X0 @ beta_hat)          # expected patents if NOT a customer\ny_pred_1 = np.exp(X1 @ beta_hat)          # expected patents if a customer\n\ndiff_vec = y_pred_1 - y_pred_0\nate_patents = diff_vec.mean()\n\nprint(f\"Average treatment effect (Blueprinty vs none): \"\n      f\"{ate_patents:.3f} extra patents per firm over 5 yrs\")\n\n\nAverage treatment effect (Blueprinty vs none): 0.713 extra patents per firm over 5 yrs\n\n\nIn short, Blueprinty’s software delivers roughly one extra patent for every five firms that adopt it, a material gain for most R&D budgets."
  },
  {
    "objectID": "projects/project3/hw2_questions (2).html#blueprinty-case-study",
    "href": "projects/project3/hw2_questions (2).html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty develops software that streamlines the blueprint drawings required for U.S. patent filings. The company’s marketing team suspects that firms using the tool see higher patent‑approval success than those that don’t. Ideally, we would compare each firm’s approval rate before and after adopting Blueprinty, but that historical view isn’t on file.\nInstead, we have a snapshot of 1,500 established engineering firms. For every company we know the number of patents awarded in the past five years, its region, age, and whether it licenses Blueprinty. By modeling these data we can isolate the impact of Blueprinty usage—controlling for basic firm attributes—and see whether the software really delivers a measurable edge in winning patents.\n\n\n\nBefore estimating any models, we first look at the raw Blueprinty dataset to understand how customer firms differ from non‑customers. This section walks through three quick diagnostics: (a) comparing patent output (b) checking whether observable firm traits—region (c) age—line up across the two groups. These checks set expectations for the Poisson regressions we will fit later.\n\n\n::: {#cell-Comparing Patent Output .cell execution_count=2}\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\ndf = pd.read_csv(\"/Users/yoonjeong_park/mysite/_data/blueprinty.csv\")   # adjust \n\n# corporate palette\nBLUE   = \"#6CA0DC\"   # customers\nPURPLE = \"#B19CD9\"   # non‑customers\n\n# ----- prepare counts -------------------------------------------------\nbins      = np.arange(df[\"patents\"].min(), df[\"patents\"].max() + 1)        # 0 … 16\ncust_cnt  = (df.loc[df[\"iscustomer\"] == 1, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\nnon_cnt   = (df.loc[df[\"iscustomer\"] == 0, \"patents\"]\n               .value_counts()\n               .reindex(bins, fill_value=0)\n               .sort_index())\n\n# ----- plot ------------------------------------------------------------\nx      = bins\nwidth  = 0.4                       # half‑bin width\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax.bar(x - width/2, cust_cnt.values,\n       width=width, color=BLUE,   label=\"Customer\")\nax.bar(x + width/2, non_cnt.values,\n       width=width, color=PURPLE, label=\"Non‑customer\")\n\n# nice, integer axes\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.set_xlabel(\"Number of patents\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Number of Patents\")\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n:::\nThe distribution of number of patents shows a clear picture:\n\nCustomers file a bit more. Most Blueprinty users sit around 4 patents and a few reach double digits.\n\nNon-customers stay low. Firms that don’t use the software pile up at 0–2 patents and rarely pass 6.\n\nStill plenty of overlap. The majority of all firms hold fewer than 6 patents, so the two groups aren’t miles apart.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nregion_counts = (\n    df\n    .groupby(['iscustomer', 'region'])\n    .size()\n    .unstack(fill_value=0)                    # rows → status, cols → region\n    .rename(index={0: 'Non-customer', 1: 'Customer'})\n)\n\n# ── make sure the rows are in the order we want ────────────────\nregion_counts_reordered = region_counts.loc[['Customer', 'Non-customer']]\n\n# ── grouped bar chart (not stacked) ────────────────────────────\nax = region_counts_reordered.T.plot(\n    kind='bar',\n    figsize=(7, 4),\n    width=0.75,                       # a touch wider than default\n    color=['#6CA0DC', '#B19CD9']      # Customer = blue, Non-customer = purple\n)\n\nplt.xlabel('Region')\nplt.ylabel('Number of firms')\nplt.title('Distribution of Regions by Customer Status')\nplt.xticks(rotation=0)\nplt.legend(title='Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bars plot say a lot about where Blueprinty’s users come from:\n\nNortheast is the hotspot. It hosts the biggest share of all firms and, notably, an even larger share of customers.\n\nEverywhere else skews non-customer. Midwest, Northwest, South, and Southwest each have many more non-customers than customers—sometimes five-to-one.\n\nRoom to grow. Those non-customer-heavy regions hint at obvious white-space for the sales team.\n\n\n\n\n\n\nAge distribution by customer status\n# ── Split ages by status ───────────────────────────────────────\nage_cust = df.loc[df.iscustomer == 1, 'age']\nage_non  = df.loc[df.iscustomer == 0, 'age']\n\n# ── Overlapping histograms ─────────────────────────────────────\nbins = np.arange(0, df['age'].max() + 5, 5)     # 5-year bins\n\nplt.figure(figsize=(7, 4))\nplt.hist(age_cust, bins=bins, alpha=0.7, color='#6CA0DC', label='Customer')\nplt.hist(age_non,  bins=bins, alpha=0.7, color='#B19CD9', label='Non-customer')\n\nplt.xlabel('Firm age (years)')\nplt.ylabel('Number of firms')\nplt.title('Age Distribution by Customer Status')\nplt.legend(title='Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nIf region separates customers from non-customers, age clearly does not:\n\nNearly identical curves. Both segments cluster in the 15-35-year window, with peaks around the mid-20s. You can barely tell one histogram from the other.\n\nFew outliers. Only a sprinkling of firms—customer or not—make it past 40 years. The extremes don’t tilt toward either side.\n\nFirm age doesn’t look like a deciding factor in adopting Blueprinty. When we estimate the Poisson model, we’ll control for age to be safe, but we don’t expect it to change the customer effect we saw in regions.\n\n\n\n\nSince our outcome, the count of patents in the last five years is a non-negative integer, the Poisson distribution is a natural starting point. We begin with the most stripped-down case: one common rate parameter, λ, for every firm and estimate it by Maximum Likelihood.\n\n\nFor a single firm\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\quad\\Longrightarrow\\quad\nf(Y_i\\mid\\lambda)\n=\\frac{e^{-\\lambda}\\,\\lambda^{Y_i}}{Y_i!}.\n\\]\nWith \\(n\\) independent firms the joint likelihood is the product of those densities:\n\\[\n\\mathcal{L}(\\lambda;\\mathbf y)\n=\\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n=e^{-n\\lambda}\\,\n  \\lambda^{\\sum_{i=1}^{n}y_i}\\Big/\\!\\prod_{i=1}^{n}y_i!.\n\\]\nThe log‑likelihood derived above is easy to translate into Python. All we need is (i) the rate parameter lmbda and (ii) an array of observed counts y.\n\nfrom scipy.special import gammaln   \n\ndef poisson_loglikelihood(lmbda: float, y: np.ndarray) -&gt; float:\n    y = np.asarray(y)\n    n = y.size\n    return -n * lmbda + y.sum() * np.log(lmbda) - gammaln(y + 1).sum()\n\n\n\n\nBefore trusting the optimiser, it’s good practice to see the curve we are maximising. Below I feed the observed patent counts into poisson_loglikelihood, evaluate it on a grid of candidate λ’s, and plot the log-likelihood. The peak should line up with the MLE we just computed.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- data & MLE from the previous step ---------------------------------\ny_data = df['patents'].values\nlambda_hat = y_data.mean()                   # analytic MLE\n\n# --- grid of λ values ---------------------------------------------------\nlam_grid = np.linspace(0.1, 8, 200)          # adjust max if needed\nloglik_vals = [poisson_loglikelihood(l, y_data) for l in lam_grid]\n\n# --- plot ---------------------------------------------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(lam_grid, loglik_vals, color='#6CA0DC', lw=2)\nplt.axvline(lambda_hat, color='#B19CD9', ls='--', lw=1.5,\n            label=f'λ̂ = {lambda_hat:.2f}')\nplt.xlabel('λ (candidate rate)')\nplt.ylabel('Log-likelihood  ℓ(λ)')\nplt.title('Log-likelihood profile for the simple Poisson model')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSingle, smooth peak. The log-likelihood rises sharply, tops out near λ̂ ≈ 3.7, and then drifts downward—exactly what we expect from a concave, well-behaved Poisson likelihood.\n\nMLE matches intuition. The peak sits almost exactly at the sample mean of the patent counts, foreshadowing the analytic result we’ll confirm next.\n\nNumerical stability. The curve is steep for very small λ but flattens around the optimum, so any reasonable optimiser will land in the same neighbourhood.\n\nNext we prove this formally: taking ∂ℓ/∂λ, setting it to zero, and showing the solution is the sample mean Ŷ = λ̂.\n\n\n\nStarting from the log-likelihood\n\\[[\n\\ell(\\lambda)\n   =-n\\lambda\n    +\\bigl(\\textstyle\\sum_{i=1}^{n}y_i\\bigr)\\log\\lambda\n    -\\sum_{i=1}^{n}\\log(y_i!)]\n\\]\ntake the first derivative:\n\\[[\\frac{\\partial \\ell}{\\partial\\lambda}\n   = -n + \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}]\\]\nSet the derivative to zero and solve for λ:\n\\[[ -n + \\frac{\\sum y_i}{\\lambda}=0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda\n  =\\frac{1}{n}\\sum_{i=1}^{n}y_i\n  =\\bar Y]\\]\nBecause the second derivative is negative, this critical point is indeed a maximum.\nWe can check this in Python as well.\n\n\nCode\ny_data = df['patents'].values\nlambda_mean = y_data.mean()          \n\nlambda_grid_peak = lambda_hat        \n\nprint(f\"Closed-form  λ̂ = {lambda_mean:.4f}\")\nprint(f\"Grid/optimiser λ̂ = {lambda_grid_peak:.4f}\")\n\n\nClosed-form  λ̂ = 3.6847\nGrid/optimiser λ̂ = 3.6847\n\n\nBoth paths land on the same estimate, confirming that the sample mean is the Maximum-Likelihood estimate for this simple Poisson model.\n\n\n\nTo close the loop we ask SciPy’s optimiser to find the λ that maximises our poisson_loglikelihood. (We tell it to minimise the negative log-likelihood.)\n\nfrom scipy.optimize import minimize_scalar\n\nneg_ll = lambda lmbda: -poisson_loglikelihood(lmbda, y_data)\nopt = minimize_scalar(neg_ll, bounds=(1e-6, 20), method='bounded')\n\nlambda_opt  = opt.x\nloglik_opt  = -opt.fun\n\nprint(f\"Optimiser  λ̂ = {lambda_opt:.4f}\")\nprint(f\"Log-lik at optimum = {loglik_opt:.2f}\")\nprint(f\"Closed-form λ̂     = {lambda_mean:.4f}\") \n\nOptimiser  λ̂ = 3.6847\nLog-lik at optimum = -3367.68\nClosed-form λ̂     = 3.6847\n\n\nThe optimiser lands at λ̂ = Ȳ and produces the same log-likelihood value we computed manually. This confirms that both analytical calculus and numerical search point to one maximum: the sample mean. With the basic Poisson model nailed down, we can now move on to richer specifications where λ varies with firm characteristics.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\nfrom scipy.special import gammaln   \n\ndef poisson_reg_loglik(beta: np.ndarray,\n                       y:   np.ndarray,\n                       X:   np.ndarray) -&gt; float:\n\n    y = np.asarray(y)\n    eta = X @ beta              \n    lam = np.exp(eta)              \n    \n    return (y * eta - lam - gammaln(y + 1)).sum()\n\nThe new function is nothing more than the “one-λ” log-likelihood from Section 2, applied row-wise with firm-specific rates:\n\nη = X @ β is the linear score for each firm.\n\nλ = exp(η) enforces positivity and makes each coefficient a multiplicative effect:\na one-unit bump in a covariate scales expected patents by exp(β_j).\n\nSumming the Poisson kernel across firms gives the total log-likelihood.\n\n\n\n\nWe now assemble the design matrix X (intercept + age + age² + region dummies + customer flag) and let SciPy’s BFGS optimiser hunt for the β̂ that maximises poisson_reg_loglik.\nBFGS also returns an approximation to the inverse Hessian, which we use as an estimate of the coefficient variance–covariance matrix.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# ── log-likelihood, gradient, Hessian ─────────────────────────\ndef pois_reg_components(beta, y, X):\n    \"\"\"Return ℓ, ∇ℓ, and −H (Fisher info) for Poisson regression.\"\"\"\n    eta = X @ beta\n    lam = np.exp(np.clip(eta, -20, 20))        # overflow guard\n    ll  = (y * eta - lam - gammaln(y + 1)).sum()\n    grad = X.T @ (y - lam)\n    hess = -(X.T * lam) @ X                    # negative definite\n    return ll, grad, hess\n\ndef neg_ll_grad_hess(beta, y, X):\n    ll, grad, hess = pois_reg_components(beta, y, X)\n    return -ll, -grad, -hess                   # minimise −ℓ\n\n# ── design matrix (intercept + covariates) ────────────────────\nX_df = pd.DataFrame({\n    'intercept' : 1.0,\n    'age'       : df['age'],\n    'age_sq'    : df['age']**2,\n    'customer'  : df['iscustomer']\n})\nX_df = pd.concat([X_df,\n                  pd.get_dummies(df['region'], drop_first=True)], axis=1)\n\ny_vec, X_mat = df['patents'].values, X_df.values\np            = X_mat.shape[1]\n\n# ── optimise using Newton–CG with analytic pieces ─────────────\ninit_beta = np.zeros(p)\n\nopt = minimize(lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[0],\n               init_beta,\n               method='Newton-CG',\n               jac=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[1],\n               hess=lambda b: neg_ll_grad_hess(b, y_vec, X_mat)[2],\n               options={'xtol': 1e-8, 'disp': False})\n\nif not opt.success:\n    raise RuntimeError(opt.message)\n\nbeta_hat = opt.x\n# covariance ≈ (−H)^−1 at optimum\n_, _, hess_opt = pois_reg_components(beta_hat, y_vec, X_mat)\ncov_hat = np.linalg.inv(-hess_opt)\nse_hat  = np.sqrt(np.diag(cov_hat))\n\nresults = (\n    pd.DataFrame({'coef': beta_hat, 'std_err': se_hat},\n                 index=X_df.columns)\n    .round(4)\n)\nresults\n\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n-0.1483\n0.1785\n\n\nage\n0.1225\n0.0135\n\n\nage_sq\n-0.0025\n0.0003\n\n\ncustomer\n0.1870\n0.0309\n\n\nNortheast\n0.0255\n0.0433\n\n\nNorthwest\n-0.0330\n0.0536\n\n\nSouth\n0.0295\n0.0526\n\n\nSouthwest\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe table below translates directly into multiplicative rate effects because our link is log-linear ( λ = exp (Xβ) ).\n\nIntercept (-0.1483).\nBaseline firm — Midwest, non-customer, age = 0 — is expected to file exp(−0.15) ≈ 0.86 patents in five years.\nAge terms (0.1225 and –0.0025).\nA one-year increase raises the log rate by 0.1225 but the negative squared term drags it down, creating an inverted-U.\nCustomer flag (0.1870).\nHolding everything else constant, Blueprinty clients file exp(0.187) ≈ 1.21 times — 21 % more** — patents than non-clients.\nRegion effects (relative to Midwest).\nAll point estimates are small (±3 %) and their standard errors are of similar magnitude → no strong regional tilt once we control for age and customer status.\n\nMost coefficients have |t| &gt; 2 (β / SE), so the age dynamics and the customer bump are statistically meaningful, whereas regional differences are not.\n\n\n\nTo be sure our hand-rolled MLE matches a mature library, we refit the exact same design matrix with statsmodels’ Poisson GLM and compare coefficients.\n\n\nCode\nimport statsmodels.api as sm\n\nglm_poiss = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = glm_poiss.fit()\n\nglm_table = (\n    pd.DataFrame({\n        'coef_glm' : fit.params,\n        'se_glm'   : fit.bse,\n        'coef_mle' : beta_hat,     \n        'se_mle'   : se_hat\n    }, index=X_df.columns)\n    .round(4)\n)\nglm_table\n\n\n\n\n\n\n\n\n\ncoef_glm\nse_glm\ncoef_mle\nse_mle\n\n\n\n\nintercept\n-0.5089\n0.1832\n-0.1483\n0.1785\n\n\nage\n0.1486\n0.0139\n0.1225\n0.0135\n\n\nage_sq\n-0.0030\n0.0003\n-0.0025\n0.0003\n\n\ncustomer\n0.2076\n0.0309\n0.1870\n0.0309\n\n\nNortheast\n0.0292\n0.0436\n0.0255\n0.0433\n\n\nNorthwest\n-0.0176\n0.0538\n-0.0330\n0.0536\n\n\nSouth\n0.0566\n0.0527\n0.0295\n0.0526\n\n\nSouthwest\n0.0506\n0.0472\n0.0308\n0.0470\n\n\n\n\n\n\n\nThe GLM check confirms our custom MLE, so we can interpret the estimates with confidence.\n\nBlueprinty pays off.\nThe customer coefficient is 0.19 (SE ≈ 0.03).\nHolding age and region constant, Blueprinty users file ((0.19) ) — about 21 % more patents than similar non-users.\nAge follows an inverted-U.\nA positive linear term (≈ 0.12) and a small negative squared term (≈ –0.0025) imply the expected patent rate peaks at (-{} / (2{^2}) ) years of firm age, then declines gently.\nRegions add little once we control for age and Blueprinty adoption.\nNortheast and Southwest carry small positive log effects (≈ 0.03–0.05), Northwest a small negative one (≈ –0.03).\nAll t-values sit near 1, so none is statistically different from the Midwest baseline at conventional levels.\nBaseline level.\nThe intercept (–0.15) translates to ((-0.15) ≈ 0.86) patents over five years for a young Midwest firm that has not licensed Blueprinty.\n\nAge dynamics and Blueprinty adoption drive most of the variation in patent output; regional location does not. For strategy, focus on signing up mid-career firms (~20–30 years old) — they sit at the productivity sweet spot and stand to gain a 20 % boost in patenting by adopting Blueprinty.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\n\nMultiplicative rate ratios are handy, but managers usually ask, “So how many extra patents will Blueprinty actually help me win?” We can answer that by predicting output for every firm twice:\n\nX₀ – same covariates as the data but force customer = 0.\n\nX₁ – identical, except force customer = 1.\n\nThe average difference in the two prediction vectors gives a back-of-the-envelope “patents gained in five years” figure.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# make copies of the design matrix\nX0 = X_mat.copy()\nX1 = X_mat.copy()\n\n# column index of the customer flag\ncust_col = list(X_df.columns).index('customer')\nX0[:, cust_col] = 0\nX1[:, cust_col] = 1\n\n# predictions under the two scenarios\ny_pred_0 = np.exp(X0 @ beta_hat)          # expected patents if NOT a customer\ny_pred_1 = np.exp(X1 @ beta_hat)          # expected patents if a customer\n\ndiff_vec = y_pred_1 - y_pred_0\nate_patents = diff_vec.mean()\n\nprint(f\"Average treatment effect (Blueprinty vs none): \"\n      f\"{ate_patents:.3f} extra patents per firm over 5 yrs\")\n\n\nAverage treatment effect (Blueprinty vs none): 0.713 extra patents per firm over 5 yrs\n\n\nIn short, Blueprinty’s software delivers roughly one extra patent for every five firms that adopt it, a material gain for most R&D budgets."
  },
  {
    "objectID": "projects/project3/hw2_questions (2).html#airbnb-case-study",
    "href": "projects/project3/hw2_questions (2).html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.\nKey variables:\n\nnumber_of_reviews – our stand-in for bookings\n\nListing “age” (days)\n\nProperty traits: room_type, beds, baths, nightly price\n\nThree 1-to-10 review scores: cleanliness, location, value\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nair = pd.read_csv('/Users/yoonjeong_park/mysite/_data/airbnb.csv')\n\n\nKey fields are the unit’s host-tenure (days), property traits (room_type, beds, baths, price) and a reviews block that we’ll treat as a proxy for bookings.\n\n\nPreprocessing dataset\n\n1. Missing Value Check\nBefore plotting anything we need to know which columns are incomplete and how big the gaps are.\n\n\nCode\n# percent missing per column\nmiss_pct = air.isna().mean().sort_values(ascending=False) * 100\nmiss_tbl = miss_pct.to_frame('pct_missing').round(1)\nmiss_tbl.head(10)        # show the ten worst offenders\n\n# quick bar chart\nmiss_tbl.plot(kind='barh', figsize=(6,4), legend=False, color='#B19CD9')\nplt.xlabel('% missing')\nplt.title('Missing data by column')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart shows that missingness is concentrated in the three review-score columns—about 25 % of listings lack a cleanliness, location, or value score. Everything else (price, room type, bathrooms, etc.) is essentially complete (&lt; 1 % missing).\n\n\n2. Build a clean working dataset\nThe missing-value scan told us that only the three review-score columns have sizable gaps. Everything else is virtually complete, so the simplest and safest move is to keep those score variables and drop the rows that are missing any of our modelling fields.\n\n\nCode\n# columns we plan to use in the model\nkeep_cols = [\n    'number_of_reviews',          \n    'room_type',\n    'bathrooms',\n    'bedrooms',\n    'price',\n    'days',                      \n    'review_scores_cleanliness',\n    'review_scores_location',\n    'review_scores_value',\n]\n\nair_cln = air.dropna(subset=keep_cols).copy()\nprint(f\"Rows kept: {air_cln.shape[0]} of {air.shape[0]} \"\n      f\"({air_cln.shape[0]/air.shape[0]:.0%})\")\n\n\nRows kept: 30160 of 40628 (74%)\n\n\nWe now have a clean dataframe (air_cln) with all necessary variables present for roughly three-quarters of the original listings. That dataset is what we’ll use for plots, feature engineering, and the Poisson model in the next steps.\n\n\n\nExploratory Data Analysis\nWe’ll start with a handful of simple plots:\n\nHistogram of nightly price (spot outliers & decide on scaling).\n\nHistogram of number_of_reviews (our bookings proxy).\n\nBox-plots of reviews by room_type (entire / private / shared).\n\nScatter of reviews vs. host tenure (days) to see if long-standing listings really do attract more business.\n\n\n\nCode\nimport seaborn as sns\n\ndf = air_cln.copy()\n# ── Trim crazy tails just for plotting ─────────────────────────\np99_price = np.percentile(df['price'], 99)\np99_days  = np.percentile(df['days'], 99)\n\ndf_plot = df[(df['price'] &lt;= p99_price) & (df['days'] &lt;= p99_days)].copy()\n\nfig, ax = plt.subplots(2, 2, figsize=(10,6))\n\n# 1. Price (under 99th-pct)\nsns.histplot(df_plot['price'], bins=40, color='#6CA0DC', ax=ax[0,0])\nax[0,0].set_title('Nightly price ($)  – 99th-pct trimmed')\n\n# 2. Reviews (log y-axis)\nsns.histplot(df_plot['number_of_reviews'], bins=40,\n             color='#B19CD9', ax=ax[0,1])\nax[0,1].set_yscale('log')\nax[0,1].set_title('Number of reviews  (log y)')\n\n# 3. Violin plot by room type\nsns.violinplot(x='room_type', y='number_of_reviews',\n               data=df_plot, cut=0, inner='quartile',\n               palette=['#6CA0DC','#B19CD9','#96d1b2'], ax=ax[1,0])\nax[1,0].set_yscale('log')\nax[1,0].set_title('Reviews by room type (log y)')\n\n# 4. Reviews vs tenure (hexbin for density)\nax[1,1].clear()                                       # wipe the old hexbin\n\nsample_idx = np.arange(0, df_plot.shape[0], 5)        # every 5th point for speed\nax[1,1].scatter(df_plot['days'].iloc[sample_idx] / 30,   # months\n                df_plot['number_of_reviews'].iloc[sample_idx],\n                s=12, alpha=0.08, color='#6CA0DC')\n\nax[1,1].set_xlabel('Months listed')\nax[1,1].set_ylabel('Reviews')\nax[1,1].set_yscale('log')\nax[1,1].set_title('Reviews vs. host tenure (log y)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNightly price (histogram).\n\nMost listings cluster between $60 and $200\nThe tail thins out quickly after $300 even after trimming the extreme 1 % → A log or “price ÷ 100” scaling will keep the coefficient sizes readable.\n\nNumber of reviews (histogram, log-y).\nThe bar on the far left reminds us that many listings have just a handful of reviews, while a small elite racks up 200 +\n→ The long right tail is exactly why a Poisson/NB count model is appropriate.\nReviews by room type (violin, log-y).\nEntire homes dominate the upper part of the distribution; private and especially shared rooms sit lower across the board\n→ We’ll keep dummy variables for these categories in the regression.\nReviews vs. host tenure (scatter, log-y).\nA clear upward cloud: listings live longer tend to accumulate more reviews, but the slope flattens after about 24 months.\n→ We expect a positive, possibly diminishing-returns coefficient on days.\n\nTogether these patterns guide the model specification we build next.\n\n\nPoisson regression\nOur plots suggest that review counts grow with listing age, fall with price, and differ by room type and review scores.\nWe encode those insights in a log-linear Poisson model:\n\\[[\n\\text{E[reviews]} \\;=\\;\n\\exp\\!\\bigl(\n\\beta_0\n+ \\beta_1 \\,\\text{months\\_listed}\n+ \\beta_2 \\,\\text{price\\_h}\n+ \\beta_3 \\,\\text{bathrooms}\n+ \\beta_4 \\,\\text{bedrooms}\n+ \\boldsymbol\\beta_5^\\top \\!\\text{scores}\n+ \\boldsymbol\\beta_6^\\top \\!\\text{room dummies}\n\\bigr)\n\\]\\]\n\n\nCode\ndf = air_cln.copy()\ndf['price_h']      = df['price'] / 100\ndf['months_listed'] = df['days'] / 30\n\nX_df = pd.concat([\n    pd.Series(1.0, index=df.index, name='intercept'),\n    df[['months_listed', 'price_h', 'bathrooms', 'bedrooms',\n        'review_scores_cleanliness', 'review_scores_location',\n        'review_scores_value']],\n    pd.get_dummies(df['room_type'], drop_first=True)  # private / shared\n], axis=1)\n\ny_vec  = df['number_of_reviews'].values\nX_mat  = X_df.values\n\n\n\npoiss_mod = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = poiss_mod.fit()\n\ncoef_tbl = (\n    pd.DataFrame({\n        'coef':  fit.params,\n        'std_err': fit.bse\n    }, index=X_df.columns)\n    .round(4)\n)\ncoef_tbl\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n3.6458\n0.0160\n\n\nmonths_listed\n0.0015\n0.0000\n\n\nprice_h\n-0.0037\n0.0009\n\n\nbathrooms\n-0.1105\n0.0038\n\n\nbedrooms\n0.0756\n0.0020\n\n\nreview_scores_cleanliness\n0.1138\n0.0015\n\n\nreview_scores_location\n-0.0809\n0.0016\n\n\nreview_scores_value\n-0.0971\n0.0018\n\n\nPrivate room\n0.0121\n0.0027\n\n\nShared room\n-0.2172\n0.0086\n\n\n\n\n\n\n\nBecause the model is log-linear, each coefficient shows the percentage change in expected review for a one-unit bump in that variable (holding the others fixed).\nHere’s what the signs and magnitudes tell us:\n\n\n\n\n\n\n\n\nVariable\nβ (≈)\nWhat it means in practice\n\n\n\n\nMonths listed\n+0.0015\nEach extra month on Airbnb lifts bookings by about 0.15 %. Over a year that compounds to ≈ 1.8 %.\n\n\nPrice (+$100)\n-0.0037\nRaising the nightly rate $100 trims bookings by ≈ 0.4 % — a very small price sensitivity in this market.\n\n\nBathrooms\n-0.11\nSurprisingly, adding a bathroom is linked to 10 % fewer reviews once price and bedrooms are held constant. Likely collinearity with luxury pricing.\n\n\nBedrooms\n+0.076\nAn extra bedroom attracts ≈ 8 % more bookings.\n\n\nCleanliness score (+1 pt)\n+0.114\nA single-point bump (on the 1–10 scale) raises bookings by ≈ 12 %. Clean units sell.\n\n\nLocation score (+1 pt)\n-0.081\nHigher “location” scores correlate with ≈ 8 % fewer reviews. (Dense tourist zones may face tougher competition.)\n\n\nValue score (+1 pt)\n-0.097\nA higher “value” score likewise shows a ≈ 9 % dip — indicating listings that over-deliver on value often compete on price and thus garner fewer stays overall.\n\n\nPrivate room vs. entire home\n+0.012\nNo real difference: booking volume is essentially the same.\n\n\nShared room vs. entire home\n-0.217\nShared rooms draw ≈ 20 % fewer bookings than entire homes.\n\n\n\n\n\nConclusion\nFor New York City Airbnbs, bookings (reviews) are driven far more by perceived quality than by headline price.\n\nAdd a bedroom → ~ 8 % more stays.\n\nKeep the place spotless → each extra cleanliness star buys ~ 12 % more bookings.\n\nStaying power counts, but only at the margin (≈ 2 % per year on the platform).\n\nA $100 price hike barely moves demand inside the common $50–$300 range.\n\nShared rooms remain a niche, drawing about 20 % fewer guests than entire homes; private rooms perform just as well as full units.\n\nTherefore, hosts should focus effort (and budget) on cleanliness and clear value rather than aggressive price cuts; consider adding sleeping capacity before adding another bathroom; and remember that patience—accruing months and reviews—pays modest but steady dividends."
  },
  {
    "objectID": "projects/project3/hw2_questions (2).html#poisson-regression",
    "href": "projects/project3/hw2_questions (2).html#poisson-regression",
    "title": "Poisson Regression Examples",
    "section": "3 Poisson regression",
    "text": "3 Poisson regression\nOur plots suggest that review counts grow with listing age, fall with price, and differ by room type and review scores.\nWe encode those insights in a log-linear Poisson model:\n\\[[\n\\text{E[reviews]} \\;=\\;\n\\exp\\!\\bigl(\n\\beta_0\n+ \\beta_1 \\,\\text{months\\_listed}\n+ \\beta_2 \\,\\text{price\\_h}\n+ \\beta_3 \\,\\text{bathrooms}\n+ \\beta_4 \\,\\text{bedrooms}\n+ \\boldsymbol\\beta_5^\\top \\!\\text{scores}\n+ \\boldsymbol\\beta_6^\\top \\!\\text{room dummies}\n\\bigr)\n\\]\\]\n\n\nCode\ndf = air_cln.copy()\ndf['price_h']      = df['price'] / 100\ndf['months_listed'] = df['days'] / 30\n\nX_df = pd.concat([\n    pd.Series(1.0, index=df.index, name='intercept'),\n    df[['months_listed', 'price_h', 'bathrooms', 'bedrooms',\n        'review_scores_cleanliness', 'review_scores_location',\n        'review_scores_value']],\n    pd.get_dummies(df['room_type'], drop_first=True)  # private / shared\n], axis=1)\n\ny_vec  = df['number_of_reviews'].values\nX_mat  = X_df.values\n\n\n\npoiss_mod = sm.GLM(y_vec, X_mat, family=sm.families.Poisson())\nfit       = poiss_mod.fit()\n\ncoef_tbl = (\n    pd.DataFrame({\n        'coef':  fit.params,\n        'std_err': fit.bse\n    }, index=X_df.columns)\n    .round(4)\n)\ncoef_tbl\n\n\n\n\n\n\n\n\ncoef\nstd_err\n\n\n\n\nintercept\n3.6458\n0.0160\n\n\nmonths_listed\n0.0015\n0.0000\n\n\nprice_h\n-0.0037\n0.0009\n\n\nbathrooms\n-0.1105\n0.0038\n\n\nbedrooms\n0.0756\n0.0020\n\n\nreview_scores_cleanliness\n0.1138\n0.0015\n\n\nreview_scores_location\n-0.0809\n0.0016\n\n\nreview_scores_value\n-0.0971\n0.0018\n\n\nPrivate room\n0.0121\n0.0027\n\n\nShared room\n-0.2172\n0.0086\n\n\n\n\n\n\n\nBecause the model is log-linear, each coefficient shows the percentage change in expected review for a one-unit bump in that variable (holding the others fixed).\nHere’s what the signs and magnitudes tell us:\n\n\n\n\n\n\n\n\nVariable\nβ (≈)\nWhat it means in practice\n\n\n\n\nMonths listed\n+0.0015\nEach extra month on Airbnb lifts bookings by about 0.15 %. Over a year that compounds to ≈ 1.8 %.\n\n\nPrice (+$100)\n-0.0037\nRaising the nightly rate $100 trims bookings by ≈ 0.4 % — a very small price sensitivity in this market.\n\n\nBathrooms\n-0.11\nSurprisingly, adding a bathroom is linked to 10 % fewer reviews once price and bedrooms are held constant. Likely collinearity with luxury pricing.\n\n\nBedrooms\n+0.076\nAn extra bedroom attracts ≈ 8 % more bookings.\n\n\nCleanliness score (+1 pt)\n+0.114\nA single-point bump (on the 1–10 scale) raises bookings by ≈ 12 %. Clean units sell.\n\n\nLocation score (+1 pt)\n-0.081\nHigher “location” scores correlate with ≈ 8 % fewer reviews. (Dense tourist zones may face tougher competition.)\n\n\nValue score (+1 pt)\n-0.097\nA higher “value” score likewise shows a ≈ 9 % dip — indicating listings that over-deliver on value often compete on price and thus garner fewer stays overall.\n\n\nPrivate room vs. entire home\n+0.012\nNo real difference: booking volume is essentially the same.\n\n\nShared room vs. entire home\n-0.217\nShared rooms draw ≈ 20 % fewer bookings than entire homes.\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  }
]